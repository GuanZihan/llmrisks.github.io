<!doctype html>
<html class="no-js" lang="en-us">
  <head>
    <meta charset="utf-8">
    <title>Week 3: Prompting and Bias | Risks (and Benefits) of Generative AI and Large Language Models</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="https://llmrisks.github.io/css/foundation.min.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/fonts.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/finite.css">
    <link rel="shortcut icon" href="/images/uva.png">  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>

    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="https://llmrisks.github.io/">Risks (and Benefits) of Generative AI and Large Language Models</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      


	    </div>
	  </div>
	    
	  
	    <div class="top-bar" id="site-menu" >	      
	      <div class="top-bar-title show-for-medium site-title">
		<a href="https://llmrisks.github.io/">Risks (and Benefits) of Generative AI and Large Language Models</a>
	      </div>
	      <div class="top-bar-left">
		<ul class="menu vertical medium-horizontal">
		  
		  
		</ul>
	      </div>
	      <div class="top-bar-right show-for-medium">
		


	      </div>
	    </div>
	  
	</nav>
      
    </header>
    
    <main>
      
<div class="row" style="padding-top: 16pt;">
  <div class="column small-12 medium-10 medium-offset-1 end large-8 large-offset-0">
    <article class="article" itemscope itemtype="http://schema.org/Article">
      
      <h1 itemprop="name">Week 3: Prompting and Bias</h1>
      <div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-09-06 00:00:00 &#43;0000 UTC" itemprop="datePublished">6 September 2023</time>
  </span>
  
</div>

      <div class="post-body" itemprop="articleBody">
        <h1 id="readings">Readings</h1>
<ol>
<li>
<p>(for Monday) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou. <a href="https://arxiv.org/abs/2201.11903"><em>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</em></a>. 2022.</p>
</li>
<li>
<p>(for Wednesday) Myra Cheng, Esin Durmus, Dan Jurafsky. <a href="https://arxiv.org/abs/2305.18189">Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models</a>. ACL 2023.</p>
</li>
</ol>
<h2 id="optional-additional-readings">Optional Additional Readings</h2>
<p><strong>Background:</strong></p>
<ul>
<li>Lilian Weng, <a href="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/"><em>Prompting Engineering</em></a>. March 2023.</li>
<li>Miles Turpin, Julian Michael, Ethan Perez, Samuel R. Bowman. <a href="https://arxiv.org/abs/2305.04388"><em>Language Models Don&rsquo;t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting</em></a>. May 2023.</li>
<li>Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh. <a href="https://arxiv.org/abs/2102.09690"><em>Calibrate Before Use: Improving Few-Shot Performance of Language Models</em></a>. ICML 2021.</li>
</ul>
<p><strong>Stereotypes and bias:</strong></p>
<ul>
<li>Yang Trista Cao, Anna Sotnikova, Hal Daumé III, Rachel Rudinger, Linda Zou.    <a href="https://aclanthology.org/2022.naacl-main.92.pdf"><em>Theory-Grounded Measurement of U.S. Social Stereotypes in English Language Models</em></a>. NAACL 2022.</li>
</ul>
<p><strong>Prompt Injection:</strong></p>
<ul>
<li>Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, Sameer Singh. <a href="https://arxiv.org/abs/1908.07125"><em>Universal Adversarial Triggers for Attacking and Analyzing NLP</em></a>. EMNLP 2019</li>
<li>Simon Willison, <a href="https://simonwillison.net/2022/Sep/12/prompt-injection/"><em>Prompt injection attacks against GPT-3</em></a>. September 2022.</li>
<li>William Zhang. <a href="https://www.robustintelligence.com/blog-posts/prompt-injection-attack-on-gpt-4"><em>Prompt Injection Attack on GPT-4</em></a>. Robust Intelligence, March 2023.</li>
</ul>
<h1 id="discussion-questions">Discussion Questions</h1>
<p>Everyone who is not in either the lead or blogging team for the week should post (in the comments below) an answer to at least one of the four questions in each section, or a substantive response to someone else&rsquo;s comment, or something interesting about the readings that is not covered by these questions.</p>
<p>Don&rsquo;t post duplicates - if others have already posted, you should read their responses before adding your own. Please post your responses to different questions as separate comments.</p>
<p>First section (1 – 4): Before <strong>5:29pm</strong> on <strong>Sunday, September 10</strong>.<br>
Second section (5 – 9): Before <strong>5:29pm</strong> on <strong>Tuesday, September 12</strong>.</p>
<h2 id="before-sunday-questions-about-chain-of-thought-prompting">Before Sunday: Questions about Chain-of-Thought Prompting</h2>
<ol>
<li>
<p>Compared to other types of prompting, do you believe that chain-of-thought prompting represents the most effective approach for enhancing the performance of LLMs? Why or why not? If not, could you propose an alternative?</p>
</li>
<li>
<p>The paper highlights several examples where chain-of-thought prompting can significantly improve its outcomes, such as in solving math problems, applying commonsense reasoning, and comprehending data. Considering these improvements, what additional capabilities do you envision for LLMs using chain-of-thought prompting?</p>
</li>
<li>
<p>Why are different language models in the experiment performing differently with chain-of-thought prompting?</p>
</li>
<li>
<p>Try some of your own experiments with prompt engineering using your favorite LLM, and report interesting results. Is what you find consistent with what you expect from the paper? Are you able to find any new prompting methods that are effective?</p>
</li>
</ol>
<h2 id="by-tuesday-questions-about-marked-personas">By Tuesday: Questions about Marked Personas</h2>
<ol start="5">
<li>
<p>The paper addresses potential harms from LLMs by identifying the underlying stereotypes present in their generated contents. Additionally, the paper offers methods to examine and measure those stereotypes. Can this approach effectively be used to diminish stereotypes and enhance fairness? What are the main limitations of the work?</p>
</li>
<li>
<p>The paper mentions racial stereotypes identified in downstream applications such as story generation. Are there other possible issues we might encounter when the racial stereotypes in LLMs become problematic after its application?</p>
</li>
<li>
<p>Much of the evaluation in this work uses a list of White and Black stereotypical attributes provided by Ghavami and Peplau (2013) as the human-written responses and compares them with the list of LLMs generated responses. This, however, does not encompass all racial backgrounds and is heavily biased by American attitudes about racial categories, and they might not distinguish between races in great detail. Do you believe there could be a notable difference when more comprehensive racial representation is incorporated? If yes, what potential differences may arise? If no, why not?</p>
</li>
<li>
<p>This work emphasizes the naturalness of the input provided to the LLM, while we have previously seen examples of eliciting harmful outputs by using less natural language. What potential benefits or risks are there in not investigating less natural inputs (e.g., prompt injection attacks including the suffix attack we saw in Week 2)? Can you suggest a less natural prompt that could reveal additional or alternate stereotypes?</p>
</li>
<li>
<p>The authors recommend transparency of bias mitigation methods, citing the benefit it could provide to researchers and practitioners. Specifically, how might researchers benefit from this? Can you foresee any negative consequences (either to researchers or the general users of these models) of this transparency?</p>
</li>
</ol>

      </div>

      <meta itemprop="wordCount" content="704">
      <meta itemprop="datePublished" content="2023-09-06">
      <meta itemprop="url" content="https://llmrisks.github.io/week3/">
    </article>

    <ul class="pagination" role="navigation" aria-label="Pagination">
      
      <li class="arrow" aria-disabled="true"><a href="https://llmrisks.github.io/week1/">&laquo; <em>Previous<span class="show-for-sr"> page</span></em>: Week 1: Introduction</a></li>
      
      
      <li class="arrow" aria-disabled="true"><a href="https://llmrisks.github.io/week2/"><em>Next<span class="show-for-sr"> page</span></em>: Week 2: Alignment&nbsp;&raquo;</a></li>
      
    </ul>

  </div>
</div>

    </main>
    
    
<footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-12 medium-6">
      <a href="//llmrisks.github.io"><b>cs 6501: Risks (and Benefits) of Generative AI</b></a><br>
      Fall 2023<br>
      <a href="//www.cs.virginia.edu">University of Virginia</a>
    </div>
    <div class="column small-14 medium-5">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="https://llmrisks.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
  </hr>
</footer>


    
    <div class="endofpage">
    </div>

    <script src="/js/jquery.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
