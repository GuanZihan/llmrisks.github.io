<!doctype html>
<html class="no-js" lang="en-us">
  <head>
	<meta name="generator" content="Hugo 0.71.0" />
    <meta charset="utf-8">
    <title>Risks (and Benefits) of Generative AI and Large Language Models</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="https://llmrisks.github.io/css/foundation.min.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/fonts.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/finite.css">
    <link rel="shortcut icon" href="/images/uva.png">  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>

    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="https://llmrisks.github.io/">Risks (and Benefits) of Generative AI and Large Language Models</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      
	      
	    </div>
	  </div>
	    
	  
	    <div class="top-bar" id="site-menu" >	      
	      <div class="top-bar-title show-for-medium site-title">
		<a href="https://llmrisks.github.io/">Risks (and Benefits) of Generative AI and Large Language Models</a>
	      </div>
	      <div class="top-bar-left">
		<ul class="menu vertical medium-horizontal">
		  
		  
		</ul>
	      </div>
	      <div class="top-bar-right show-for-medium">
		
	         <p class="groupstyle">Computer Science</br>University of Virginia</p>
		
	      </div>
	    </div>
	  
	</nav>
      
    </header>
    
    <main>
      





   <div class="container">       
   <div class="sidebar">
   <p>
University of Virginia<Br>
cs6501 Fall 2023<br>
Risks and Benefits of Generative AI and LLMs
</p>
   <p>
   <p>
     
<a href="/syllabus"><b>Syllabus</b></a></br>
  <a href="/schedule"><b>Schedule</b></a></br>
<a href="/readings"><b>Readings and Topics</b></a></br>
   <p></p>
   <a href="https://github.com/llmrisks/discussions/discussions/">Discussions</a><br>
   


<p>
<p></p>



</p>

   <p>
   <b><a href="/post/">Recent Posts</a></b>

   
   <div class="posttitle">
      <a href="/week3/">Week 3: Prompting and Bias</a>


   </div>
   
   <div class="posttitle">
      <a href="/week1/">Week 1: Introduction</a>


   </div>
   
   <div class="posttitle">
      <a href="/discussions/">Github Discussions</a>


   </div>
   
   <div class="posttitle">
      <a href="/class0/">Class 0: Getting Organized</a>


   </div>
   
   <div class="posttitle">
      <a href="/updates/">Updates</a>


   </div>
   
   <div class="posttitle">
      <a href="/survey/">Welcome Survey</a>


   </div>
   
   <div class="posttitle">
      <a href="/welcome/">Welcome to the LLM Risks Seminar</a>


   </div>
   
   <div class="posttitle">
     &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
     <a href="/post/"><em>More...</em></a>
   </div>
   
   </p>
   <p>

   </p>
<p>
 

   </div>


    <div class="content">

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    


    
    
    <h1><a href="/week3/">Week 3: Prompting and Bias</a></h1>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-09-06 00:00:00 &#43;0000 UTC" itemprop="datePublished">6 September 2023</time>
  </span>
  
</div>

      <div class="post-body" itemprop="articleBody">
        <h1 id="readings">Readings</h1>
<ol>
<li>
<p>(for Monday) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou. <a href="https://arxiv.org/abs/2201.11903"><em>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</em></a>. 2022.</p>
</li>
<li>
<p>(for Wednesday) Myra Cheng, Esin Durmus, Dan Jurafsky. <a href="https://arxiv.org/abs/2305.18189">Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models</a>. ACL 2023.</p>
</li>
</ol>
<h2 id="optional-additional-readings">Optional Additional Readings</h2>
<p><strong>Background:</strong></p>
<ul>
<li>Lilian Weng, <a href="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/"><em>Prompting Engineering</em></a>. March 2023.</li>
<li>Miles Turpin, Julian Michael, Ethan Perez, Samuel R. Bowman. <a href="https://arxiv.org/abs/2305.04388"><em>Language Models Don&rsquo;t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting</em></a>. May 2023.</li>
<li>Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh. <a href="https://arxiv.org/abs/2102.09690"><em>Calibrate Before Use: Improving Few-Shot Performance of Language Models</em></a>. ICML 2021.</li>
</ul>
<p><strong>Stereotypes and bias:</strong></p>
<ul>
<li>Yang Trista Cao, Anna Sotnikova, Hal Daumé III, Rachel Rudinger, Linda Zou.    <a href="https://aclanthology.org/2022.naacl-main.92.pdf"><em>Theory-Grounded Measurement of U.S. Social Stereotypes in English Language Models</em></a>. NAACL 2022.</li>
</ul>
<p><strong>Prompt Injection:</strong></p>
<ul>
<li>Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, Sameer Singh. <a href="https://arxiv.org/abs/1908.07125"><em>Universal Adversarial Triggers for Attacking and Analyzing NLP</em></a>. EMNLP 2019</li>
<li>Simon Willison, <a href="https://simonwillison.net/2022/Sep/12/prompt-injection/"><em>Prompt injection attacks against GPT-3</em></a>. September 2022.</li>
<li>William Zhang. <a href="https://www.robustintelligence.com/blog-posts/prompt-injection-attack-on-gpt-4"><em>Prompt Injection Attack on GPT-4</em></a>. Robust Intelligence, March 2023.</li>
</ul>
<h1 id="discussion-questions">Discussion Questions</h1>
<p>Everyone who is not in either the lead or blogging team for the week should post (in the comments below) an answer to at least one of the four questions in each section, or a substantive response to someone else&rsquo;s comment, or something interesting about the readings that is not covered by these questions.</p>
<p>Don&rsquo;t post duplicates - if others have already posted, you should read their responses before adding your own. Please post your responses to different questions as separate comments.</p>
<p>First section (1 – 4): Before <strong>5:29pm</strong> on <strong>Sunday, September 10</strong>.<br>
Second section (5 – 9): Before <strong>5:29pm</strong> on <strong>Tuesday, September 12</strong>.</p>
<h2 id="before-sunday-questions-about-chain-of-thought-prompting">Before Sunday: Questions about Chain-of-Thought Prompting</h2>
<ol>
<li>
<p>Compared to other types of prompting, do you believe that chain-of-thought prompting represents the most effective approach for enhancing the performance of LLMs? Why or why not? If not, could you propose an alternative?</p>
</li>
<li>
<p>The paper highlights several examples where chain-of-thought prompting can significantly improve its outcomes, such as in solving math problems, applying commonsense reasoning, and comprehending data. Considering these improvements, what additional capabilities do you envision for LLMs using chain-of-thought prompting?</p>
</li>
<li>
<p>Why are different language models in the experiment performing differently with chain-of-thought prompting?</p>
</li>
<li>
<p>Try some of your own experiments with prompt engineering using your favorite LLM, and report interesting results. Is what you find consistent with what you expect from the paper? Are you able to find any new prompting methods that are effective?</p>
</li>
</ol>
<h2 id="by-tuesday-questions-about-marked-personas">By Tuesday: Questions about Marked Personas</h2>
<ol start="5">
<li>
<p>The paper addresses potential harms from LLMs by identifying the underlying stereotypes present in their generated contents. Additionally, the paper offers methods to examine and measure those stereotypes. Can this approach effectively be used to diminish stereotypes and enhance fairness? What are the main limitations of the work?</p>
</li>
<li>
<p>The paper mentions racial stereotypes identified in downstream applications such as story generation. Are there other possible issues we might encounter when the racial stereotypes in LLMs become problematic after its application?</p>
</li>
<li>
<p>Much of the evaluation in this work uses a list of White and Black stereotypical attributes provided by Ghavami and Peplau (2013) as the human-written responses and compares them with the list of LLMs generated responses. This, however, does not encompass all racial backgrounds and is heavily biased by American attitudes about racial categories, and they might not distinguish between races in great detail. Do you believe there could be a notable difference when more comprehensive racial representation is incorporated? If yes, what potential differences may arise? If no, why not?</p>
</li>
<li>
<p>This work emphasizes the naturalness of the input provided to the LLM, while we have previously seen examples of eliciting harmful outputs by using less natural language. What potential benefits or risks are there in not investigating less natural inputs (e.g., prompt injection attacks including the suffix attack we saw in Week 2)? Can you suggest a less natural prompt that could reveal additional or alternate stereotypes?</p>
</li>
<li>
<p>The authors recommend transparency of bias mitigation methods, citing the benefit it could provide to researchers and practitioners. Specifically, how might researchers benefit from this? Can you foresee any negative consequences (either to researchers or the general users of these models) of this transparency?</p>
</li>
</ol>

      </div>
<hr class="post-separator"></hr>

    
    <h1><a href="/week1/">Week 1: Introduction</a></h1>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-09-03 00:00:00 &#43;0000 UTC" itemprop="datePublished">3 September 2023</time>
  </span>
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>(see bottom for assigned readings and questions)</p>
<h1 id="attention-transformers-and-bert">Attention, Transformers, and BERT</h1>
<p><strong>Monday, 28 August</strong></p>
<p>Transformers<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> are a class of deep learning models that have
revolutionized the field of natural language processing (NLP) and
various other domains. The concept of transformers originated as an
attempt to address the limitations of traditional recurrent neural
networks (RNNs) in sequential data processing. Here&rsquo;s an overview of
transformers&rsquo; evolution and significance.</p>
<h2 id="background-and-origin">Background and Origin</h2>
<p>RNNs<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> were one of the earliest models used for sequence-based tasks in machine learning. They processed input tokens one after another and used their internal memory to capture dependencies in the sequence. The following figure gives an illustration of the RNN architecture.</p>
<center>
<a href="/images/rnn.png"><img src="/images/rnn.png" width="60%"></a><br>
<p>RNN (<a href="https://cs231n.github.io/rnn/">Image Source</a>)</p>
</center>
<p><strong>Limitations of RNNs.</strong>
Despite many improvements over this basic architecture, RNNs have the following shortcomings:</p>
<ul>
<li>RNNs struggle with long sequences. It only keeps recent information but looses long-term memory.</li>
<li>RNNs suffer from vanishing gradients<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. In this, the gradients that are used to update the model become very small during back propagation, leading the RNNs to learn nothing from training.</li>
</ul>
<p><strong>Introduction of LSTMs.</strong>
Long Short-Term Memory (LSTM)<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> networks were then introduced to address the vanishing gradient problem in RNNs. LSTMs had memory cells and gating mechanisms that allowed them to capture long-term memories more effectively. While LSTMs improved memory retention, they were still computationally expensive and slow to train, especially on large datasets.</p>
<p><strong>Attention Mechanism.</strong>
The attention mechanism<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup><sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup><sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> was introduced as a way to help models focus on relevant parts of the input sequence when generating output. This addressed the memory issues that plagued previous models.
Attention mechanisms allowed models to weigh the importance of different input tokens when making predictions or encoding information. In essence, it enables the model to focus selectively on relevant parts of the input sequence while disregarding less pertinent ones. In practice, attention mechanism can be categorized into self-attention and multi-head attention based on the number of heads used in the attention structure.</p>
<h2 id="the-transformer-model">The Transformer Model</h2>
<p>The transformer architecture, introduced by Vaswani et al. (2017) <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, marked a significant advance in NLP. It used self-attention mechanisms to process input tokens in parallel and capture contextual information more effectively.
Transformers broke down sentences into smaller parts and learned statistical relationships between these parts to understand meaning and generate responses.
The model utilized input embeddings to represent words and positional encodings to address the lack of inherent sequence information.
The core innovation was the self-attention mechanism, which allowed tokens to consider their relationships with all other tokens in the sequence</p>
<p><strong>Benefits of Transformers.</strong>
Transformers can capture complex contextual relationships in language, making them highly effective for a wide range of NLP tasks.
The parallel processing capabilities of transformers, enabled by self-attention, drastically improved training efficiency and reduced the vanishing gradient problem.</p>
<p><strong>Mathematical Foundations.</strong>
Transformers involve mathematical representations of words and their relationships. The model learns to establish connections between words based on their contextual importance.</p>
<p><strong>Crucial Role in NLP.</strong>
Transformers play a crucial role in capturing the meaning of words and sentences<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup><sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>, allowing for more accurate and contextually relevant outputs in various NLP tasks.
In summary, transformers, with their innovative attention mechanisms, have significantly advanced the field of NLP by enabling efficient processing of sequences, capturing context effectively, and achieving state-of-the-art performance on a variety of tasks.</p>
<p><strong>Advancements in Transformers.</strong>
One significant advancement of transformers over previous models like LSTMs and RNNs is their ability to handle long-range dependencies and capture contextual information more effectively. Transformers achieve this through self-attention and multi-head attention. This allows them to process input tokens in parallel, rather than sequentially, leading to improved efficiency and performance. However, a drawback could be increased computational complexity due to the parallel processing, especially in multi-head attention.</p>
<p><strong>Positional Encodings.</strong>
The use of positional encodings in transformers helps address the lack of inherent positional information in their architecture. This enables transformers to handle sequential data effectively without relying solely on the order of tokens. The benefits include scalability and the ability to handle longer sequences, but a potential drawback is that these positional encodings might not fully capture complex positional relationships in very long sequences.</p>
<p><strong>Self-Attention and Multi-Head Attention.</strong>
Self-attention is a useful mechanism that allows each token to consider the relationships between all other tokens in a sequence. While it provides a more nuanced understanding of input, it can be computationally expensive. The use of multi-head attention further enhances the model&rsquo;s ability to capture different types of dependencies in the data. The number of attention heads (e.g., 8 in BERT) is a balance between performance and complexity. Too few or too many heads can result in suboptimal performance. More details about self-attention and multi-head attention can be found in <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>.</p>
<p><strong>Context and Answers in Activities.</strong>
Let’s do some activity now!</p>
<pre><code class="language-angular2html" data-lang="angular2html">I used to ___ 

Yesterday, I went to ___

It is raining ___
</code></pre><p>The context given in the activities influences the answers provided. More context leads to more accurate responses. This highlights how models like BERT benefit from bidirectional attention, as they can consider both preceding and succeeding words when generating predictions.</p>
<h2 id="bert-bidirectional-transformers">BERT: Bidirectional Transformers</h2>
<p><strong>BERT&rsquo;s Design and Limitations.</strong>
BERT<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup> uses bidirectional attention and masking to enable it to capture context from both sides of a word. The masking during training helps the model learn to predict words in context, simulating its real-world usage. While BERT&rsquo;s design was successful, it does require a substantial amount of training data and resources. Its application may be more focused on tasks such as sentiment analysis, named entity recognition, and Question answering, while GPT is better at handling tasks such as content creation, text summarization, and machine translation<sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>.</p>
<center>
<a href="/images/bert.png"><img src="/images/bert.png" width="60%"></a><br>
<p><a href="https://production-media.paperswithcode.com/methods/new_BERT_Overall.jpg">Image Source</a></p>
</center>
<p><strong>Future Intent of BERT Authors.</strong>
The authors of BERT might not have fully anticipated its exact future use and impact. While they likely foresaw its usefulness, the swift and extensive adoption of language models across diverse applications likely surpassed their expectations. The increasing accessibility and scalability of technology likely contributed to this rapid adoption. As mentioned by the professor in class, the decision to publish something in industry (and at Google in particular) often depends on its perceived commercial value. If Google were aware of the future commercial value of transformers and the methods introduced by BERT, they may not have published these papers openly (although this is purely speculation without any knowledge of the internal process that might have been followed to publish these papers).</p>
<h2 id="discussion-questions">Discussion Questions</h2>
<blockquote>
<p>Q: <em>What makes language models different from transformers?</em></p>
</blockquote>
<p>A language model encompasses various models that understand language, whereas transformers represent a specific architecture. Language models are tailored for natural languages, while transformers have broader applications. For example, transformers can be utilized in tasks beyond language processing, such as predicting protein structures from genomic sequences (as done by AlphaFold).</p>
<blockquote>
<p>Q: <em>Why was BERT published in 2019, inspiring large language models, and why have GPT models continued to improve while BERT&rsquo;s advancements seem comparatively limited?</em></p>
</blockquote>
<p>Decoder models, responsible for generating content, boast applications that are both visible and instantly captivating to the public. Examples like chatbots, story generators, and models from the GPT series showcase this ability by producing human-like text. This immediate allure likely fuels increased research and investment. Due to the inherent challenges in producing coherent and contextually appropriate outputs, generative tasks have garnered significant research attention. Additionally, decoder models, especially transformers like GPT-2<sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup> and GPT-3<sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>, excel in transfer learning, allowing pre-trained models to be fine-tuned for specific tasks, highlighting their remarkable adaptability.</p>
<blockquote>
<p>Q: <em>Why use 8-headers in the transformer architecture?</em></p>
</blockquote>
<p>The decision to use 8 attention heads is a deliberate choice that strikes a balance between complexity and performance. Having more attention heads can capture more intricate relationships but increases computational demands, whereas fewer heads might not capture as much detail.</p>
<blockquote>
<p>Q: <em>BERT employs bidirectional context to pretrain its embeddings, but there is debate about whether this approach genuinely captures the entirety of language context?</em></p>
</blockquote>
<p>The debate arises from the fact that while bidirectional context is powerful, it might not always capture more complex contextual relationships, such as those involving long-range dependencies or nuanced interactions between distant words. Some argue that models with other architectures or training techniques might better capture such intricate language nuances.</p>
<h1 id="wednesday-training-llms-risks-and-rewards">Wednesday: Training LLMs, Risks and Rewards</h1>
<p>In the second class discussion, the team talked about LLMs and tried to make sense of how they&rsquo;re trained, where they get their knowledge, and where they&rsquo;re used. Here&rsquo;s what they found out.</p>
<p><strong>How do LLMs become so clever?</strong></p>
<p>Before LLMs become language wizards, they need to be trained. The crucial question is where they acquire their knowledge.</p>
<p>LLMs need lots and lots of information to learn from. They look at stuff like internet articles, books, and even Wikipedia. But there&rsquo;s a catch. They have a clean-up crew called &ldquo;C4&rdquo; to make sure the information is tidy and reliable.</p>
<p>Training LLMs requires potent computational resources, such as Graphics Processing Units (GPUs). Computationally-expensive large-scale training, while crucial for enhancing their capabilities, involves substantial energy consumption, which, depending on how it is produces may emit large amounts of carbon dioxide.</p>
<p>Transitioning to the practical applications of these language models, LLMs excel in diverse domains<sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>. They can undergo meticulous fine-tuning to perform specialized tasks, ranging from aiding in customer service to content generation for websites. Furthermore, these models exhibit the ability to adapt and learn from feedback, mirroring human learning processes.</p>
<h3 id="risks-and-rewards">Risks and Rewards</h3>
<p>In our class discussion, we had a friendly debate about LLMs. Some students thought they were fantastic because they can boost productivity, assist with learning, and bridge gaps between people. They even saw LLMs as potential problem solvers for biases in the human world.</p>
<p>But others had concerns. They worried about things like LLMs being too mysterious (like a black box), how they could influence the way people think, and the risks of false information and deep fakes. Some even thought that LLMs might detrimentally impact human intelligence and creativity.</p>
<p>In our debate, there were some interesting points made:</p>
<p><strong>Benefits Group.</strong></p>
<ul>
<li>LLMs can enhance creativity and accelerate tasks.</li>
<li>They have the potential to facilitate understanding and learning.</li>
<li>Utilizing LLMs may streamline the search for ideas.</li>
<li>LLMs offer a tool for uncovering and rectifying biases within our human society. Unlike human biases, there are technical approaches to mitigate biases in models.</li>
</ul>
<p><strong>Risks Group.</strong></p>
<ul>
<li>Concerns were expressed regarding LLMs&rsquo; opacity and complexity, making them challenging to comprehend.</li>
<li>Apprehensions were raised about LLMs potentially exerting detrimental influences on human cognition and societal dynamics.</li>
<li>LLMs are ripe for potential abuses in their ability to generate convincing false information cheaply.</li>
<li>The potential impact of LLMs on human intelligence and creativity was a topic of contemplation.</li>
</ul>
<p>After the debate, both sides had a chance to respond:</p>
<p><strong>Benefits Group Rebuttals.</strong></p>
<ul>
<li>Advocates pointed out that ongoing research aims to enhance the transparency of LLMs, reducing their resemblance to black boxes.</li>
<li>They highlighted collaborative efforts directed at the improvement of LLMs.</li>
<li>The significance and potential of LLMs in domains such as medicine and engineering was emphasized.</li>
<li>Although the ability of generative AI to produce art in the style of an artist is damaging to the career of that artist, it is overall beneficial to society, enabling many others to create desired images.</li>
<li>Addressing economic concerns, proponents saw LLMs as catalysts for the creation of new employment opportunities and enhancers of human creativity.</li>
</ul>
<p><strong>Risks Group Rebuttals.</strong></p>
<ul>
<li>They noted the existence of translation models and the priority of fairness in AI.</li>
<li>Advocates asserted that LLMs can serve as tools to identify and mitigate societal biases.</li>
<li>The point was made that AI can complement, rather than supplant, human creativity.</li>
<li>Although generating AI art may have immediate benefits to its users, it has long term risks to our culture and society if individuals are no longer able to make a living as artists or find the motivation to learn difficult skills.</li>
</ul>
<p><strong>Wrapping It Up.</strong>
So, there you have it, a peek into the world of Large Language Models and the lively debate about their pros and cons. As you explore the world of LLMs, remember that they have the power to be amazing tools, but they also come with responsibilities. Use them wisely, consider their impact on our world, and keep the discussion going!</p>
<h1 id="readings">Readings</h1>
<p><a href="https://stanford-cs324.github.io/winter2022/lectures/introduction/">Introduction to Large Language Models</a> (from Stanford course)</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. <a href="https://arxiv.org/abs/1706.03762"><em>Attention Is All You Need</em></a>. <a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>. NeurIPS 2017.</p>
<p>These two blog posts by Jay Alammar are not required readings but may be helpful for understanding attention and Transformers:</p>
<ul>
<li><a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)</a></li>
<li><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></li>
</ul>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. <a href="https://aclanthology.org/N19-1423/"><em>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</em></a>. ACL 2019.</p>
<p>Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zac Kenton, Sasha Brown, Will Hawkins, Tom Stepleton, Courtney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks, William Isaac, Sean Legassick, Geoffrey Irving, Iason Gabriel. <a href="https://arxiv.org/abs/2112.04359"><em>Ethical and social risks of harm from Language
Models</em></a> DeepMind, 2021. <a href="https://arxiv.org/abs/2112.04359">https://arxiv.org/abs/2112.04359</a></p>
<h2 id="optional-additional-readings">Optional Additional Readings:</h2>
<p>Center for Research on Foundation Models (CRFM) at the Stanford Institute for Human-Centered Artificial Intelligence (HAI). <a href="https://arxiv.org/abs/2108.07258"><em>On the Opportunities and Risks of Foundation Models</em></a></p>
<p>Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. <a href="https://aclanthology.org/N18-1202"><em>Deep Contextualized Word Representations</em></a>. Conference of the North American Chapter of the Association for Computational Linguistics, 2018.</p>
<p>GPT1: Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. <a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf"><em>Improving language understanding by generative pre-training</em></a>. 2018.</p>
<p>GPT2: Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"><em>Language models are unsupervised multitask learners</em></a>, 2019.</p>
<p>GPT3: Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. <a href="https://arxiv.org/abs/2005.14165"><em>Language models are few-shot learners</em></a>, 2020.</p>
<h1 id="discussion-questions-1">Discussion Questions</h1>
<p>Before <strong>5:29pm on Sunday, August 27</strong>, everyone who is not in either the lead or blogging team for the week should post (in the comments below) an answer to at least one of these three questions in the first section (1&ndash;3) <strong>and</strong> one of the questions in the section section (4&ndash;7), or a substantive response to someone else&rsquo;s comment, or something interesting about the readings that is not covered by these questions.</p>
<p>Don&rsquo;t post duplicates - if others have already posted, you should read their responses before adding your own.</p>
<p>Questions about &ldquo;Attention is All You Need&rdquo; and &ldquo;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&rdquo;:</p>
<ol>
<li>
<p>Many things in the paper (especially &ldquo;Attention is All You Need&rdquo;) seem mysterious and arbitrary. Identify one design decision described in the paper that seems arbitrary, and possible alternatives. If you can, hypothesize on why the one the authors made was selected and worked.</p>
</li>
<li>
<p>What were the key insights that led to the Transformers/BERT design?</p>
</li>
<li>
<p>What is something you don&rsquo;t understand in the paper?</p>
</li>
</ol>
<p>===</p>
<p>Questions about &ldquo;Ethical and social risks of harm from Language Models&rdquo;</p>
<ol start="4">
<li>
<p>The paper identifies six main risk areas and 21 specific risks. Do you agree with their choices? What are important risks that are not included in their list?</p>
</li>
<li>
<p>The authors are at a company (DeepMind, part of Google/Alphabet). How might their company setting have influenced the
way they consider risks?</p>
</li>
<li>
<p>This was written in December 2021 (DALL-E was released in January 2021; ChatGPT was released in November 2022; GPT-4 was released in March 2023). What has changed since then that would have impacted perception of these risks?</p>
</li>
<li>
<p>Because training and operating servers typically requires fresh water and fossil fuels, how should we think about the environmental harms associated with LLMs?</p>
</li>
<li>
<p>The near and long-term impact of LLMs on employment is hard to predict. What jobs do you think are vulnerable to LLMs beyond the (seemingly) obvious ones mentioned in the paper? What are some jobs you think will be most resilient to advances in AI?</p>
</li>
</ol>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., &hellip; &amp; Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30. <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>Sherstinsky, A. (2020). Fundamentals of recurrent neural network (RNN) and long short-term memory (LSTM) network. Physica D: Nonlinear Phenomena, 404, 132306. <a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>Pascanu, R., Mikolov, T., &amp; Bengio, Y. (2013, May). On the difficulty of training recurrent neural networks. In International conference on machine learning (pp. 1310-1318). Pmlr. <a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p>Hochreiter, S., &amp; Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780. <a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5" role="doc-endnote">
<p>Mnih, V., Heess, N., &amp; Graves, A. (2014). Recurrent models of visual attention. Advances in neural information processing systems, 27. <a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6" role="doc-endnote">
<p>Bahdanau, D., Cho, K., &amp; Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473. <a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7" role="doc-endnote">
<p>Lin, T., Wang, Y., Liu, X., &amp; Qiu, X. (2022). A survey of transformers. AI Open. <a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8" role="doc-endnote">
<p>Khan, S., Naseer, M., Hayat, M., Zamir, S. W., Khan, F. S., &amp; Shah, M. (2022). Transformers in vision: A survey. ACM computing surveys (CSUR), 54(10s), 1-41. <a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9" role="doc-endnote">
<p>Karim, R. (2023, January 2). Illustrated: Self-attention. Medium. <a href="https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a">https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a</a> <a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10" role="doc-endnote">
<p>Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. <a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11" role="doc-endnote">
<p>Ahmad, K. (2023b, April 26). GPT vs. Bert: What are the differences between the two most popular language models?. MUO. <a href="https://www.makeuseof.com/gpt-vs-bert/">https://www.makeuseof.com/gpt-vs-bert/</a> <a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12" role="doc-endnote">
<p>Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., &amp; Sutskever, I. (2019). Language models are unsupervised multitask learners. OpenAI blog, 1(8), 9. <a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13" role="doc-endnote">
<p>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., &hellip; &amp; Amodei, D. (2020). Language models are few-shot learners. Advances in neural information processing systems, 33, 1877-1901. <a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14" role="doc-endnote">
<p>Yang, J., Jin, H., Tang, R., Han, X., Feng, Q., Jiang, H., &hellip; &amp; Hu, X. (2023). Harnessing the power of llms in practice: A survey on chatgpt and beyond. arXiv preprint arXiv:2304.13712. <a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

      </div>
<hr class="post-separator"></hr>

    
    <h1><a href="/discussions/">Github Discussions</a></h1>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-08-25 00:00:00 &#43;0000 UTC" itemprop="datePublished">25 August 2023</time>
  </span>
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>Everyone should have received an invitation to the github discussions
site, and be able to see the posts there and submit your own posts and
comments. If you didn&rsquo;t get this invitation, it was probably blocked
by the email system. Try visiting:</p>
<p><a href="https://github.com/orgs/llmrisks/invitation">https://github.com/orgs/llmrisks/invitation</a></p>
<p>(while logged into the github account you listed on your form).</p>
<p>Once you&rsquo;ve accepted the invitation, you should be able to visit
<a href="https://github.com/llmrisks/discussions/discussions/2">https://github.com/llmrisks/discussions/discussions/2</a>
(the now-finalized discussion post for Week 1), and contribute to the
discussions there.</p>

      </div>
<hr class="post-separator"></hr>

    
    <h1><a href="/class0/">Class 0: Getting Organized</a></h1>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-08-23 00:00:00 &#43;0000 UTC" itemprop="datePublished">23 August 2023</time>
  </span>
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>I&rsquo;ve updated the <a href="/schedule">Schedule</a> and <a href="/weeklyschedule">Bi-Weekly
Schedule</a> based on the discussions today.</p>
<p>The plan is below:</p>
<table class="schedule">
<thead>
<tr>
<th align="center">Week</th>
<th align="center">Lead Team</th>
<th align="center">Blogging Team</th>
<th align="center">Everyone Else</th>
</tr>
</thead>
<tbody>
<tr>
<td>Two Weeks Before</td>
<td>
Come up with idea for the week and planned readings, send to me by
5:29pm on Tuesday (2 weeks - 1 day before)
</td>
<td>-</td>
<td>
-
</td>
</tr>
<tr>
<td>Week Before</td>
<td>
Post plan and questions in github discussions by no later than 9am Wednesday;
prepare for leading meetings
</td>
<td>
<b>Prepare plan for blogging</b> (how you will divide workload, collaborative tools for taking notes and writing)
</td>
<td>
Read/do materials and <b>respond</b> to preparation questions in github discussions (by <b>5:29pm Sunday</b>)
</td>
</tr>
<tr>
<td>Week of Leading Meetings</td>
<td>
<b>Lead interesting, engaging, and illuminating meetings!</b><br>
Aim to include activities, discussions, whiteboard presentations, etc., not just showing powerpoint slides.
</td>
<td>
<B>Take notes</b> to prepare to write blog; participate actively in class meetings
</td>
<td>
Participate actively in class meetings
</td>
</tr>
<tr>
<td>
Week After</td>
<td>
Help blogging team with materials, answering questions
</td>
<td>
<a href="https://llmrisks.github.io/blogging/"><b>Write blog summary</b></a>, submit PR
</td>
<td>
Provide feedback on blog summary
</td>
</tr>
</tbody>
</table>
<p>For this week, Team 1 should finalize the post by this Thursday
(5:29pm), and Team 2 should send me (email to <A
href="mailto:evans@virginia.edu"><a href="mailto:evans@virginia.edu">evans@virginia.edu</a></a>) the plan for
week 2 by Sunday, 27 August and have materials ready for posting by
the next Wednesday.</p>
<p>The teams have been posted at <a href="https://github.com/llmrisks/discussions/blob/main/teams.md">https://github.com/llmrisks/discussions/blob/main/teams.md</a> (visible only to the class). Everyone should have gotten an invite to the github organization, but if not please check with me.</p>

      </div>
<hr class="post-separator"></hr>

    
    <h1><a href="/updates/">Updates</a></h1>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-08-21 00:00:00 &#43;0000 UTC" itemprop="datePublished">21 August 2023</time>
  </span>
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>Some materials have been posted on the course site:</p>
<ul>
<li><a href="/syllabus">Syllabus</a></li>
<li><a href="/schedule">Schedule</a> (you will find out which team you are on at the first class Wednesday)</li>
<li><a href="/readings">Readings and Topics</a> (a start on a list of some potential readings and topics that we might want to cover)</li>
</ul>
<center>
<a href="/images/dallephdseminar.png"><img src="/images/dallephdseminar.png" width="60%"></a><br>
<font size="-1">Dall-E Prompt: "comic style drawing of a phd seminar on AI"</font>
</center>

      </div>
<hr class="post-separator"></hr>

    
    <h1><a href="/survey/">Welcome Survey</a></h1>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-08-17 00:00:00 &#43;0000 UTC" itemprop="datePublished">17 August 2023</time>
  </span>
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>Please submit this welcome survey before 8:59pm on Monday, August 21:</p>
<center>
<p><a href="https://forms.gle/dxhFmJH7WRs32s1ZA">https://forms.gle/dxhFmJH7WRs32s1ZA</a></p>
</center>
<p>Your answers won&rsquo;t be shared publicly, but I will use the responses to
the survey to plan the seminar, including forming teams, and may share
some aggregate and anonymized results and anonymized quotes from the
surveys.</p>

      </div>
<hr class="post-separator"></hr>

    
    <h1><a href="/welcome/">Welcome to the LLM Risks Seminar</a></h1>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-05-26 00:00:00 &#43;0000 UTC" itemprop="datePublished">26 May 2023</time>
  </span>
  
</div>

      <div class="post-body" itemprop="articleBody">
        <center>
<img src="/images/planningcourse.png" witdh="60%"><br>
<p><a href="https://poe.com/s/9bOFQnPCmJn537ZVEhuM">Full Transcript</a></p>
</center>
<h2 id="seminar-plan">Seminar Plan</h2>
<p>The actual seminar won&rsquo;t be fully planned by GPT-4, but more
information on it won&rsquo;t be available until later.</p>
<p>I&rsquo;m expecting the structure and format to that combines aspects of
<a href="https://secml.github.io/syllabus/">this seminar on adversarial machine
learning</a> and <a href="https://csethics.github.io/syllabus/">this course on
computing ethics</a>, but with a
topic focused on learning as much as we can about the potential for
both good and harm from generative AI (including large language
models) and things we can do (mostly technically, but including
policy) to mitigate the harms.</p>
<p><strong>Expected Background:</strong> Students are not required to have prior
background in machine learing or security, but will be expected to
learn whatever background they need on these topics mostly on their
own. The seminar is open to ambitious undergraduate students and
research-focused graduate students with interests in machine
learning, privacy, fairness, security, and related
topics. Instructor permission is required to enroll, and decisions
about enrollment will be based on what you are able to bring to the
seminar.</p>
<p><strong>Seminar Format:</strong> The details will be worked out later, but the
basic structure will divide the class into three or four teams
(somewhat like what was done , each with set responsibilities for
each week of the seminar. One team will be responsible for leading
the seminar, including selecting readings/viewings/activities for
the rest of the course and leading discussions in class (with help
from the instructor). Another team will be responsible for writing a
&ldquo;blog&rdquo; that summarizes the content of the week.</p>
<p><strong>Content:</strong> We expect the content will be a mix of technical
background papers, recent research papers, and less formal writings
and videos. Although we will focus on understanding technical
aspects of the issues, we will also consider non-technical ones
including societal impacts and legal and policy aspects.</p>
<h2 id="heading"></h2>
<h2 id="readings">Readings</h2>
<p>Some initial ideas for course readings will be posted, but it will be
largely up to the student teams leading to select good readings for
the topics to consider.</p>
<center>
<img src="/images/readings.png" witdh="30%"><br>
<p><a href="https://poe.com/s/9bOFQnPCmJn537ZVEhuM">Full Transcript</a></p>
</center>

      </div>
<hr class="post-separator"></hr>

    


    <footer>
      <nav>
	<a href="/post/" class="button hollow primary">All Posts</a>
      </nav>
    </footer>

</div>
</div>


    </main>
    
    
    <footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-12 medium-6">
      <a href="//llmrisks.github.io"><b>cs 6501: Risks (and Benefits) of Generative AI</b></a><br>
      Fall 2023<br>
      <a href="//www.cs.virginia.edu">University of Virginia</a>
    </div>
    <div class="column small-14 medium-5">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="https://llmrisks.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
  </hr>
</footer>

    
    
    <div class="endofpage">
    </div>

    <script src="/js/jquery.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
