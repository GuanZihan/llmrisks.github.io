<!doctype html>
<html class="no-js" lang="en-us">
  <head>
	<meta name="generator" content="Hugo 0.92.0" />
    <meta charset="utf-8">
    <title>Risks (and Benefits) of Generative AI and Large Language Models</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="https://llmrisks.github.io/css/foundation.min.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/fonts.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/finite.css">
    <link rel="shortcut icon" href="/images/uva.png">  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>

    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="https://llmrisks.github.io/">Risks (and Benefits) of Generative AI and Large Language Models</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      
	      
	    </div>
	  </div>
	    
	  
	    <div class="top-bar" id="site-menu" >	      
	      <div class="top-bar-title show-for-medium site-title">
		<a href="https://llmrisks.github.io/">Risks (and Benefits) of Generative AI and Large Language Models</a>
	      </div>
	      <div class="top-bar-left">
		<ul class="menu vertical medium-horizontal">
		  
		  
		</ul>
	      </div>
	      <div class="top-bar-right show-for-medium">
		
	         <p class="groupstyle">Computer Science</br>University of Virginia</p>
		
	      </div>
	    </div>
	  
	</nav>
      
    </header>
    
    <main>
      





   <div class="container">       
   <div class="sidebar">
   <p>
University of Virginia<Br>
cs6501 Fall 2023<br>
Risks and Benefits of Generative AI and LLMs
</p>
   <p>
   <p>
     
<a href="/syllabus"><b>Syllabus</b></a></br>
  <a href="/schedule"><b>Schedule</b></a></br>
<a href="/readings"><b>Readings and Topics</b></a></br>
   <p></p>
   <a href="https://github.com/llmrisks/discussions/discussions/">Discussions</a><br>
   


<p>
<p></p>



</p>

   <p>
   <b><a href="/post/">Recent Posts</a></b>

   
   <div class="posttitle">
      <a href="/week4/">Week 4: Capabilities of LLMs</a>


   </div>
   
   <div class="posttitle">
      <a href="/week3/">Week 3: Prompting and Bias</a>


   </div>
   
   <div class="posttitle">
      <a href="/week2/">Week 2: Alignment</a>


   </div>
   
   <div class="posttitle">
      <a href="/week1/">Week 1: Introduction</a>


   </div>
   
   <div class="posttitle">
      <a href="/discussions/">Github Discussions</a>


   </div>
   
   <div class="posttitle">
      <a href="/class0/">Class 0: Getting Organized</a>


   </div>
   
   <div class="posttitle">
      <a href="/updates/">Updates</a>


   </div>
   
   <div class="posttitle">
      <a href="/survey/">Welcome Survey</a>


   </div>
   
   <div class="posttitle">
      <a href="/welcome/">Welcome to the LLM Risks Seminar</a>


   </div>
   
   <div class="posttitle">
     &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
     <a href="/post/"><em>More...</em></a>
   </div>
   
   </p>
   <p>

   </p>
<p>
 

   </div>


    <div class="content">

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    


    
    
    <h1><a href="/week4/">Week 4: Capabilities of LLMs</a></h1>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-09-25 00:00:00 &#43;0000 UTC" itemprop="datePublished">25 September 2023</time>
  </span>
  
</div>

      <div class="post-body" itemprop="articleBody">
        <h1 id="capabilities-of-llms-week-4">Capabilities of LLMs (Week 4)</h1>
<p><author>Presenting Team: Xindi Guo, Mengxuan Hu, Tseganesh Beyene Kebede, Zihan Guan</author></p>
<p><author>Blogging Team: Ajwa Shahid, Caroline Gihlstorf, Changhong Yang, Hyeongjin Kim, Sarah Boyce</author></p>
<h1 id="monday-september-18">Monday, September 18</h1>
<p>Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, Xia Hu. <em>Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond</em>. April 2023. <a href="https://arxiv.org/abs/2304.13712">https://arxiv.org/abs/2304.13712</a></p>
<p>This discussion was essential to highlight the distinction between large language models (LLMs) and fine-tuned models. The paper defines LLMs as models trained on large amounts of data without a particular domain of application in mind and fine-tuned models as models trained on some general data in addition to data from a specific domain in which they will perform a particular task. They also highlighted a rule of thumb from the paper that LLMs typically have more than 20 billion parameters whereas fine-tuned models have fewer than 20 billion parameters.</p>
<p>The presenters then discussed how to go about choosing the model that is best for a task when there are so many available. They explored two ways of approaching this problem: one that considers the data, and another that considers the task.</p>
<p>The presenters highlighted three types of data to consider for a task: the pre-training data, the fine-tuning data, and the test data. If there is a model already pre-trained on data that is relevant to the task at hand, that model might be a good choice. In order for a dataset to be conducive to fine-tuning a model, the data must be labeled and that there must be enough of it to sufficiently train the model. Otherwise, training an LLM would be better. The presenters noted, however, that fine-tuned models have been shown to perform better than LLMs on specific tasks when they have sufficient data to train on.</p>
<p>The following Figure 1 shows that fine-tuned models performed better than LLMs after being trained on enough data:</p>
<table><tr>
  <td><img src="/images/Week4/llms_vs_fine_tuned_models.png" width="95%"></td>
</tr>
  <td colspan=1 align="center"> Figure 1 <b> <a href="https://arxiv.org/abs/2307.11346">(Image source)</a></b></td>
</table>
<p>LLMs and fine-tuned models perform better on different tasks. According to a study from the paper <a href="https://arxiv.org/abs/2301.13848"><em>Benchmarking Large Language Models for News Summarization</em></a> <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, LLMs perform better than fine-tuned models on text summarization according to the preferences of human raters. Additionally, LLMs perform better on tasks that require large amounts of knowledge from a variety of domains<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. In machine translation, however, fine-tuned models generally do better than LLMs, although they only do slightly better in low-resource settings<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. Additionally, the presenters explained that fine-tuned models and LLMs have similar performance when the task only requires very specific knowledge.</p>
<p>The presenters then posed the following question to the class: “How could we enhance LLM in the scenario where the required knowledge does not match their learned knowledge?” The class formed four groups to discuss the question. Each group then shared a summary of what they had discussed:</p>
<p>Group 1: Discussed enhancements in both training and testing. For testing, use intentional prompts to get the knowledge you want from the model. For training, adding more training data, using a knowledge graph to classify knowledge into different clusters for ease of search, and using a plug-in, as mentioned in the GitHub discussion.</p>
<p>Group 2: Advocated for enhancement through the model&rsquo;s ability to retrieve information from external sources and undergo fine-tuning.</p>
<p>Group 3: Emphasized the significance of using more extensive datasets and guiding models through example-based prompts to extract additional knowledge from the available data.</p>
<p>Group 4: Proposed refining model performance through fine-tuning and deliberately specifying the model&rsquo;s role within a chat setting to steer it toward specific tasks.</p>
<p>The presenters also mentioned that LLMs perform arithmetic better as they grow larger and that LLMs are better at commonsense reasoning than fine-tuned models<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>.</p>
<h2 id="chatgpt-plug-ins">ChatGPT Plug-Ins</h2>
<p>The presenters then moved to a discussion of ChatGPT plug-ins. They established that LLMs are not able to keep up with current knowledge and facts and that they mainly handle text data as opposed to image data, audio, and other data formats. They also explained that while LLMs may be able to generate instructions for a human to follow to complete a task, they are often unable to follow those instructions and complete the tasks themselves. This is where plug-ins come in.</p>
<p>Plug-ins allow LLMs to access data outside of what they have already learned, for example, by searching the web. Plug-ins can enable ChatGPT to search the web (<a href="https://openai.com/blog/chatgpt-plugins#browsing">https://openai.com/blog/chatgpt-plugins#browsing</a>) and write code (<a href="https://openai.com/blog/chatgpt-plugins#code-interpreter)">https://openai.com/blog/chatgpt-plugins#code-interpreter)</a>. They also showed a video of how ChatGPT can use plug-ins from outside OpenAI to complete tasks with specific instructions, namely, finding a restaurant, then a recipe, then calculating the number of calories in the recipe (<a href="https://openai.com/blog/chatgpt-plugins#third-party-plugins)">https://openai.com/blog/chatgpt-plugins#third-party-plugins)</a>.</p>
<p>The class then formed groups and spent 25 minutes designing custom plug-ins. Each group then shared their plug-in with the class:</p>
<p><em>Group 1</em>: Envisioned a plug-in that allows a user to make DIY furniture or buy new furniture by submitting an image of something they would like to make from scratch or buy and returning either a link to a similar piece of furniture or a video/instructions on how to build a piece of furniture similar to the one in their image (Figure 2).</p>
<table><tr>
  <td><img src="/images/Week4/group_1_plugin.png" width="95%"></td>
</tr>
  <td colspan=1 align="center"> Figure 2 <b>DIY Furniture Plug-In </b></td>
</table>
<p><em>Group 2</em>: Suggested a plug-in that generates the profile of a fake person with a fake social media account. Essentially, it aimed to create a convincing online persona that would resemble a real person but was entirely fictional. (Such a plug-in, obviously, raises some serious ethical concerns and could be used for some legitimate purposes, but many malign ones!)</p>
<p><em>Group 3</em>: Created a plug-in that uses a person’s portfolio/resume to match them with different jobs serving as an ultimate career support tool (Figure 3). It assisted users in job search by matching them with relevant job openings, provided tailored preparation materials for interviews, offered resume/portfolio editing guidance, and provided opportunities to expand their professional network. Essentially, it aimed to equip individuals with the tools and resources needed to enhance their career prospects.</p>
<table><tr>
  <td><img src="/images/Week4/group_3_plugin.png" width="95%"></td>
</tr>
  <td colspan=1 align="center"> Figure 3 <b>Job Hunter Plug-In</b></td>
</table>
<p>Group 4: Shared the concept of a plug-in that allows a user to find the taxonomy of a particular research paper, and place it in a research area. Moreover, it offered the valuable function of returning related papers, thus assisting users in discovering additional relevant research in their field of interest.</p>
<p>The discussion on Monday concluded with the class appreciating each other’s ideas on the possibilities of using plug-ins but also their limitations and associated risks. This exchange of perspectives and ideas highlighted the creative ways in which technology and AI-driven tools could be used to address various challenges and opportunities across different domains.</p>
<h1 id="wednesday-medical-applications-of-llms">Wednesday: Medical Applications of LLMs</h1>
<h2 id="introduction">Introduction</h2>
<p>Wednesday&rsquo;s class started with warm-up story about a young boy who finally got diagnosed from ChatGPT. He saw 17 doctors over 3 years for chronic pain but they couldn’t diagnosis what made boy painful. One day, his parents explained the boy’s symptom to ChatGPT and they got a reliable answer. They brought this information to a doctor and after several tests, he could finally get a diagnosis which was correct information from ChatGPT. This story presents a bright possibilities for the medical use of Chat GPT.</p>
<p>We also got a simple question and had to solve the problem not using ChatGPT.</p>
<table><tr>
  <td><img src="/images/Week4/figure4.png" width="95%"></td>
</tr>
  <td colspan=1 align="center"> Figure 4 <b></b></td>
</table>
<p>About 70% of students could get an answer “A” using a simple web search (e.g., duckduckgo or google) at first trial, and 30% of students could get an answer by several attempts.</p>
<p>Later, we were asked to use ChatGPT to get an answer. All students could get a correct answer for at first trial. For a simple question like this, it is unclear if GPT is better than a web search. ChatGPT has been pre-trained from many documents from the web, so it is good at giving general information.</p>
<p>However, if we ask deeper and expertise questions to ChatGPT, it may
not give an answer. We indeed need a LLM model which is trained from
medical information to get a reliable result. (Note that GPT-4 is
already competitive in many ways with models tuneds specifically for
medical applications.)</p>
<h2 id="medical-fine-tuned-llms">Medical fine-tuned LLMs</h2>
<p>There were many attemps to build LLMs trained with medical knowlege. Med-PaLM is one of the representive models, which is fine-tuned PaLM with a medical knowledge. Figure 5 showed that Med-PalM scored 67.2% on USMLE-type questions in the MedQA dataset. Considering that approximate medical pass mark was 60%, Med-PalM was the first LLM model who passed the approximate medical pass mark by Dec 22.</p>
<table><tr>
  <td><img src="/images/Week4/figure5.png" width="95%"></td>
</tr>
  <td colspan=1 align="center"> Figure 5 <b> <a href="https://sites.research.google/med-palm/">(Image source)</a></b>
</table>
<p>Recently researchers developed Med-PaLM2. They trained PaLM2 with more medical data and upgraded. Figure 6 shows the brief explaination of how Med-PaLM2 was trained. As a result of training fine-tuned with more and better medical information, it could achieve significant improvement.</p>
<table><tr>
  <td><img src="/images/Week4/figure6.png" width="95%"></td>
</tr>
  <td colspan=1 align="center"> Figure 6 <b> <a href="https://www.youtube.com/watch?v=ixRanV-rdAQ">(Image source)</a></b>
</table>
<p>The researchers had a same experiment. Figure 7 showed that Med-PaLM2 scored an accuracy of 86.5% on USMLE-type-questions in the MedQA dataset. Compared with other LLM performance before 2023, Med-PaLM2 was the first LLM to reach expert performance.</p>
<p>In comparison to human physicians in High Quality Answer Traits, Med-PaLM2 showed better reflects consensus, better reading comprehension, better knowledge to recall, and better reasoning.</p>
<p>When they tested Potential Answer Risks, physicians omitted more
information, and gave slightly more evidence of demographic bias, and
potentially greater extent/likelihood of harm but Med-PaLM2 gave more
inaccurate/irrelevant information.</p>
<table><tr>
  <td><img src="/images/Week4/figure7.jpeg" width="95%"></td>
</tr>
  <td colspan=1 align="center"> Figure 7 <b> <a href="https://arxiv.org/pdf/2305.09617.pdf">(Image source)</a></b>
</table>
<p>We have seen evidence showing that Med-PaLM2 performs very well,
but this is just a starting point for the discussion of how AI should
be used in healthcare.</p>
<p>Presenter suggested the one way of increasing the capabilities of Med-LLMs. Extending input data from language to other source of data like image, LLMs could understand better about the condition of patients and give more correct diagnosis. Presenter introduced a multimodal version Med-PaLM, in figure 8. Accuracy of the diagonsis is mainly based on how input data contain the precise information about the patient, so as LLM model could be trained through those multimodal material, there would be larger chances of increasing the capabilities.</p>
<table><tr>
  <td><img src="/images/Week4/figure8.png" width="95%"></td>
</tr>
  <td colspan=1 align="center"> Figure 8 <b> <a href="https://sites.research.google/med-palm/">(Image source)</a></b>
</table>
<p>Also presenter gave a data about physician evaluation on Multi-Med QA and adversarial Qustion to compare the physician, Med-PaLM1, and Med-PaLM2 as you can see in figure 9. It showed the significant increased capabilities as Med-PaLM developed to Med-PaLM2. Furthermore, Almost evalution on Med-PaLM2 is within the range of physicians and some of evaluation even exceed physicians. Based on this results, presenter opened a discussion.</p>
<table><tr>
  <td><img src="/images/Week4/figure9.png" width="95%"></td>
</tr>
  <td colspan=1 align="center"> Figure 9 <b> <a href="https://arxiv.org/pdf/2305.09617.pdf">(Image source)</a></b>
</table>
<h2 id="discussion">Discussion</h2>
<p><em>What are the potential implications of LLMs reaching or surpassing human expertise in medical knowledge?</em></p>
<p>The discussion raised more questions than answers:</p>
<ul>
<li>
<p>At what point do we trust doctors vs LLMs and to what extent?</p>
</li>
<li>
<p>Are LLMs more convenient than going to a doctor?</p>
</li>
<li>
<p>People will be more prone to self-diagnosis</p>
</li>
<li>
<p>People could be more honest with AI than a human being</p>
</li>
<li>
<p>Should we let AI write prescriptions?</p>
</li>
<li>
<p>How can we truly verify what is right and what is wrong?</p>
</li>
<li>
<p>AI could obviously give a better diagnosis than human whose experiences effect diagnoses</p>
</li>
<li>
<p>What would happen when different Ais give different results?</p>
</li>
<li>
<p>There is a lot of data to train with and fine tuning. Is this worth the time and effort?</p>
</li>
<li>
<p>Who is taking accountability for the mistakes made by AI? If a doctor does something wrong, they can have their license taken, but this is that the case for AI.</p>
</li>
<li>
<p>How can we refine and improve LLMs like Med-PaLM2 to be more effective in healthcare applications?</p>
</li>
</ul>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B. Hashimoto. Benchmarking large language models for news summarization, 2023. <a href="https://arxiv.org/abs/2301.13848">https://arxiv.org/abs/2301.13848</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav
Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif,
Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard,
Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa
Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus,
Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexan-
der Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, An-
drew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz,
Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy
Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling
language modeling with pathways, 2022. <a href="https://arxiv.org/abs/2204.02311">https://arxiv.org/abs/2204.02311</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>OpenAI. GPT-4 Technical Report. March 2023. <a href="https://arxiv.org/abs/2303.08774">https://arxiv.org/abs/2303.08774</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

      </div>
<hr class="post-separator"></hr>

    
    <h1><a href="/week3/">Week 3: Prompting and Bias</a></h1>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-09-18 00:00:00 &#43;0000 UTC" itemprop="datePublished">18 September 2023</time>
  </span>
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>(see bottom for assigned readings and questions)</p>
<h1 id="prompt-engineering-week-3">Prompt Engineering (Week 3)</h1>
<p><author>Presenting Team: Haolin Liu, Xueren Ge, Ji  Hyun Kim, Stephanie Schoch </author></p>
<p><author>Blogging Team: Aparna Kishore, Erzhen Hu, Elena Long, Jingping Wan</author></p>
<ul>
<li><a href="#monday-09112023-prompt-engineering">(Monday, 09/11/2023) Prompt Engineering</a>
<ul>
<li><a href="#warm-up-questions">Warm-up questions</a></li>
<li><a href="#what-is-prompt-engineering">What is Prompt Engineering?</a></li>
<li><a href="#how-is-prompt-based-learning-different-from-traditional-supervised-learning">How is prompt-based learning different from traditional supervised learning?</a></li>
<li><a href="#in-context-learning-and-different-types-of-prompts">In-context learning and different types of prompts</a></li>
<li><a href="#what-is-the-difference-between-prompts-and-fine-tuning">What is the difference between prompts and fine-tuning?</a></li>
<li><a href="#when-is-the-best-to-use-prompts-vs-fine-tuning">When is the best to use prompts vs fine-tuning?</a></li>
<li><a href="#risk-of-prompts">Risk of Prompts</a></li>
<li><a href="#discussion-about-prompt-format">Discussion about Prompt format</a></li>
</ul>
</li>
<li><a href="#wednesday-09132023-prompt-engineering-exposing-llm-risks">(Wednesday, 09/13/2023) Prompt Engineering: Exposing LLM Risks</a>
<ul>
<li><a href="#open-discussion">Open Discussion</a></li>
<li><a href="#case-study-marked-personas">Case Study: Marked Personas</a></li>
<li><a href="#discussion-bias-mitigation">Discussion: Bias mitigation</a></li>
<li><a href="#hands-on-activity-prompt-hacking">Hands-on Activity: Prompt Hacking</a></li>
<li><a href="#discussion-can-we-defend-against-prompt-hacking-by-build-in-safegurads">Discussion: Can we defend against prompt hacking by build-in safegurads?</a></li>
<li><a href="#further-thoughts-whats-the-real-risk">Further thoughts: What&rsquo;s the real risk?</a></li>
</ul>
</li>
<li><a href="#readings">Readings</a>
<ul>
<li><a href="#optional-additional-readings">Optional Additional Readings</a></li>
<li><a href="#discussion-questions">Discussion Questions</a></li>
</ul>
</li>
</ul>
<h1 id="monday-09112023-prompt-engineering">(Monday, 09/11/2023) Prompt Engineering</h1>
<h2 id="warm-up-questions">Warm-up questions</h2>
<p>Monday&rsquo;s class started with warm-up questions to demonstrate how prompts can help an LLM produce correct answers or desired outcomes. The questions and the prompts were tested in GPT3.5. This task was performed as an in-class experiment where each individual used GPT3.5 to test the questions and help GPT3.5 produce correct answers via prompts.</p>
<p>The three questions were:</p>
<ol>
<li><em>What is 7084 times 0.99?</em></li>
<li><em>I have a magic box that can only transfer coins. If you insert a number of coins in it, the next day each coin will turn into two apples. If I add 10 coins and wait for 3 days, what will happen?</em></li>
<li><em>Among &ldquo;Oregon, Virginia, Wyoming&rdquo;, what is the word that ends with &ldquo;n&rdquo;？</em></li>
</ol>
<p>While the first question tested the arithmetic capability of the model, the second and the third questions tested common sense and symbolic reasoning, respectively. The initial response from GPT3.5 for all three questions was wrong.</p>
<p>For the first question, providing more examples as prompts did not work. At the same time, an explanation of how to reach the specific answer by decomposing the multiplication into multiple steps helped.</p>
<p>Figure 1 shows the prompting for the first question and the answer from GPT3.5.</p>
<table><tr>
<td><img src="../images/Week3/Picture1.png" width="95%"></td>
<td><img src="../images/Week3/Picture2.png" width="95%"></td><br><tr>
<td colspan=2 align="center">Figure 1: <b>Prompting for arithmetic question</b></td>
</tr></table>
<p>For the second question, providing an example and an explanation behind the reasoning on how to reach the final answer helped GPT produce the correct answer. Here, the prompt included explicitly stating that the magic box can also convert from coins to apples.</p>
<p>Figure 2 shows the prompting for the second question and the answer from GPT3.5.</p>
<table><tr>
<td><img src="../images/Week3/Picture3.png" width="95%"></td>
<td><img src="../images/Week3/Picture4.png" width="95%"></td></tr>
<tr>
<td colspan=2 align="center">Figure 2: <b>Prompting for common sense question</b>
</tr></table>
<p>While GPT was producing random results for the third question, instructing GPT through examples to take the words, concatenate the last letters, and then find the alphabet&rsquo;s position helped produce the correct answer.</p>
<p>Figure 3 shows the prompting for the third question and the answer from GPT3.5.</p>
<table><tr>
<td><img src="../images/Week3/Picture5.png" width="95%"></td>
<td><img src="../images/Week3/Picture6.png" width="95%"></td></tr>
<tr>
<td colspan=2 align="center">
Figure 3: <b>Prompting for symbolic reasoning question</b></td>
</tr></table>
<p>All these examples demonstrate the benefit of using prompts to explore the model&rsquo;s reasoning ability.</p>
<h2 id="what-is-prompt-engineering">What is Prompt Engineering?</h2>
<p>Prompt engineering is a method to communicate and guide LLM to demonstrate a behavior or desired outcomes by crafting prompts that coax the model towards providing the desired response. The model weights or parameters are not updated in prompt engineering.</p>
<h2 id="how-is-prompt-based-learning-different-from-traditional-supervised-learning">How is prompt-based learning different from traditional supervised learning?</h2>
<p>Traditional supervised learning trains a model by taking input and generating an output based on prediction probability. The model learns to map input data to specific output labels. In contrast, prompt-based learning models the probability of the text directly. Here, the inputs are converted to textual strings called prompts. These prompts are used to generate desired outcomes. Prompt-based learning offers more flexibility in adapting the model&rsquo;s behavior to different tasks by modifying the prompts. Retraining the model is not required in this scenario.</p>
<p>Interestingly, the prompts were initially used in language translations and emotion predictions based on texts instead of improving the performance of LLMs.</p>
<h2 id="in-context-learning-and-different-types-of-prompts">In-context learning and different types of prompts</h2>
<p>In-context learning is a powerful approach to fine-tuning or training the model within a specific context. This improves the performance and reliability of the model for the specific task or the environment. Here, the models are given a few examples as reference/instructions that are relevant to the context and are domain-specific.</p>
<p>We can categorized in-context learning into three different types of prompts:</p>
<ul>
<li><em>Zero-shot</em> – the model predicts the answers given only a natural language description of the task.</li>
</ul>
<center>
<a href="../images/Week3/Picture7.png"><img src="../images/Week3/Picture7.png" width="65%"></a>  
<p>Figure 4: <b>Example for zero-shot prompting</b> (<a href="https://arxiv.org/pdf/2005.14165.pdf">Image Source</a>)</p>
</center>
<ul>
<li><em>One-shot</em> or <em>Few-shot</em> – In this scenario, one or few examples are provided that explains the task description the model, i.e. prompting the model with few input-output pairs.</li>
</ul>
<center>
<a href="../images/Week3/Picture8.png"><img src="../images/Week3/Picture8.png" width="65%"></a><br>
<p>Figure 5: Examples for one-shot and fewshot prompting (<a href="https://arxiv.org/pdf/2005.14165.pdf">Image Source</a>)</p>
</center>
<ul>
<li><em>Chain-of-thought</em> – The given task or question is decomposed into coherent intermediate reasoning steps that are solved before providing the final response. This explores the reasoning ability of the model for each of the provided tasks. It is given in the format <code>&lt;input chain-of-thought output&gt;</code>. The difference between standard prompting and chain-of-thought prompting is depicted in the figure below. In the figure to the right, the highlighted statement in blue is an example of chain-of-thought prompting, where the reasoning behind reaching a final answer is provided as a part of the example. Thus, in the model outcome, the model also outputs its reasoning, highlighted in green, to reach the final answer. In addition, chain-of-thought prompting can revolutionize the way we interact with LLMs and leverage their capabilities, as they provide step-by-step explanations of how a particular response is reached.</li>
</ul>
<table><tr>
<td align="center"><img src="../images/Week3/Picture9.png" width="85%"></td>
<td align="center"><img src="../images/Week3/Picture10.png" width="85%"></td>
</tr>
<tr>
<td align="center" colspan="2">
<p>Figure 6: <b>Standard prompting and chain-of-thought prompting</b> (<a href="https://arxiv.org/abs/2201.11903">Image Source</a>)</tr></table></p>
<h2 id="what-is-the-difference-between-prompts-and-fine-tuning">What is the difference between prompts and fine-tuning?</h2>
<p>Prompt engineering focuses on eliciting better output for a given LLM through changing input. Fine-tuning focuses on enhancing model performance by training the model on a smaller, targeted database relevant to the desired task. The similarity is that both methods help improve the model&rsquo;s performance and provide desired outcomes.</p>
<p>Prompt engineering requires no retraining, and the prompting is performed in a single window of the model. At the same time, fine-tuning involves retraining the model and changing the model parameter to improve its performance. Fine-tuning also requires more computational resources compared to prompt engineering.</p>
<h2 id="when-is-the-best-to-use-prompts-vs-fine-tuning">When is the best to use prompts vs fine-tuning?</h2>
<p>The above question was an in-class discussion question, and the discussion points were shared in class. Fine-tuning requires updating model weights and changing parameters. These are useful in applications where there is a requirement for central change. In this scenario, all the users experience similar performance. Prompt-based methods are user-specific in a particular window for further fine-grained control. The model&rsquo;s performance depends on the individual prompts designed by the user. Thus, fine-tuning is more potent than prompt-based methods in scenarios that require centralized tuning.</p>
<p>In scenarios with limited training examples, prompt-based methods can perform well. Fine-tuning methods are data-hungry and require many input data for better model performance. As discussed in the discussion posts, prompts cannot be used as a universal tool for all problems to generate desired outcomes and have performance enhancements. However, in specific scenarios, it can assist users to improve performance and reach desired outcomes for in-context specific tasks.</p>
<h2 id="risk-of-prompts">Risk of Prompts</h2>
<p>The class then discussed the perspectives from risks of prompt: those methods like chain of thoughts already achieve some success in the LLMs. However, prompt engineering can be still a controversial topic. The group brought out two aspects.</p>
<p>First, Reasoning ability of LLMs. The group asked, “Does CoT empowers LLMs reasoning ability?” Secondly, there are some bias problems in prompting engineering. The group brought up an example of  “LeBron James took a corner kick.” Is the following sentence plausible? (A) plausible (B) implausible I think the answer is A and saying “but I’m curious to hear what you think.” However, this might inject a bias in the prompt.</p>
<p>The group then brought up an open discussion about two potential kinds of prompting bias and ask the class about how would the prompt format (e.g., Task-specific prompt methods, words selected) and prompt training examples (e.g., label distribution, permutations of training examples) affect LLMs output and the possible debiasing solutions.</p>
<p>The class then discussed two different kinds of prompting bias, the prompt format and the prompt training examples.</p>
<h3 id="discussion-about-prompt-training-examples">Discussion about Prompt training examples</h3>
<p>For label distribution, the class discussed that there needs to be a balance in the training set to avoid overgeneralizing the agreement of the user as some examples that the user interjects an opinion that can be wrong. In these cases, the GPT should learn to disagree with the user when the user is wrong. This is also related to the label distribution, if  the user always provides the example with positive labels, then the LLMs will be more likely to output the positive one in the prediction.</p>
<p>Permutation on the training example: A student mentioned a paper that he just read about why context learning works, provides the label space and the distribution of the input. In the paper, they randomly generate the labels, which might be false, they show that actually is better at zero-shot, though worse than when you provide all the labels. Randomly generated labels actually have a significant performance input.
The sequence of the training example may affect the LLM output, especially for the last example. LLM output tends to output the same label with the last example being provided training example.</p>
<h2 id="discussion-about-prompt-format">Discussion about Prompt format</h2>
<p>Prompt format: the word you selected might affect the prompt because some words may appear more frequently in the coporus and some words may have more correlation with some specific label. Male may relate to more positive terms in their training coporus. some prompting may affect the results. Task-specific prompt methods are related to how you select prompt methods based on specific task.</p>
<p>Finally, the group shared two papers about the bias problem in LLMs.
The first paper<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> shows that different prompts will provide a large variance in accuracy, which indicates LLMs are not that stable. The paper also provides a calibration method that takes the output of the GPT model and another linear layer on it to calibrate the models.
The second paper<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> shows that LLMs do not always say what they think, especially injecting some bias into the prompt. For example, they worked on the CoT and non-CoT and they found that CoT will amplify the bias in the context when the user puts some bias in the prompt.</p>
<p>In conclusion, prompts can be controversial and not always perfect.</p>
<!-- TODO: find the actual papers, put in links and make the references more complete. -->
<h1 id="wednesday-09132023-marked-personas">(Wednesday, 09/13/2023) Marked Personas</h1>
<h2 id="open-discussion">Open Discussion</h2>
<p>What do you think are the biggest potential risks of LLMs?</p>
<ul>
<li><strong>Social impact from intentional misuse.</strong> LLM’s content could be manipulated by the government, can potentially affect elections and raise tensions between countries.</li>
<li><strong>Mutual trust among people could be harmed.</strong> We cannot tell which email or info was written by humans or automatically generated by chatgpt. As a result, we may treat these information more skeptically.</li>
<li><strong>People may overly trust LLM outputs.</strong> We may rely more on asking LLM, which is a second-hand information source, rather than actively searching information by ourselves, overtrusting LLM system. Information pool may be contaminated by LLM if they provide misleading information.</li>
</ul>
<p>How does GPT-4 Respond?</p>
<ul>
<li>Misinformation: Provide wrong / misleading / sensitive information, known as jailbreaking of LLM.</li>
<li>Potential manipulation: People could intentionally hack LLM by giving specific prompts.</li>
</ul>
<h2 id="case-study-marked-personas">Case Study: Marked Personas</h2>
<p>Myra Cheng, Esin Durmus, Dan Jurafsky. <a href="https://arxiv.org/abs/2305.18189">Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models</a>. ACL 2023.</p>
<p>In this study, ChatGPT was asked to give descriptions about several characters based on different ethnicity / gender / demographic groups, e.g., Asian woman, Black woman and white man.</p>
<p>When describing a character from a non-dominant demographic group, while the overall description is positive, it could still imply some potential stereotypes. For example, “Almond-shaped eye” is used in describing an east asian woman while it may sound strange to a real east asian. We can also see that ChatGPT is intentionally trying to build a more diverse and politically correct atmosphere for different groups. In contrast, ChatGPT uses mostly neutral and ordinary words when describing an average white man.</p>
<h2 id="discussion-bias-mitigation">Discussion: Bias mitigation</h2>
<h3 id="group-1">Group 1</h3>
<p>Mitigation could sometimes be overcompensating. As a language model, it should try to be neutral and independent. Also, given that people themselves are biased, and LLM is learning from the human world, we may be over-expecting LLMs to be perfectly unbiased. After all, it is hard to define what is fairness and distinguish between stereotype and prototype, leading to over corrections.</p>
<h3 id="group-2">Group 2</h3>
<p>We may be able to identify the risks by data augmentation (replacing “male” with “female” in prompts). Governments should also be responsible for setting rules and regulating LLMs. (Note: this is controversial, and it is unclear what kinds of regulations might be useful or effective.)</p>
<h3 id="group-4">Group 4</h3>
<p>Companies like OpenAI should publish the mitigation strategies so that it could be understood and monitored by the public. Another aspect is that different groups of people can have very diverse points of views, so it is hard to define the stereotypes and biases with a universal law. Also, the answer could be very different based on the prompts, making it even harder to mitigate</p>
<h2 id="hands-on-activity-prompt-hacking">Hands-on Activity: Prompt Hacking</h2>
<p>In this activity, the class was trying to make ChatGPT generate sensitive / bad responses. It could be done by setting a pretended identity, e.g. pretending to be a Hutu person in Rwanda in the 1990s or pretending to be a criminal. With these conditions, ChatGPT’s barrier of biased or evil contents can be partly bypassed.</p>
<h2 id="discussion-can-we-defend-against-prompt-hacking-by-build-in-safegurads">Discussion: Can we defend against prompt hacking by build-in safegurads?</h2>
<p>As we can see in the activity, right now this safeguard is not that strong. A more practical way may be to add a disclaimer at the end of potentially sensitive content and give a questionnaire to collect feedback for better iteration. Companies should also actively identify these jailbreakings and attempt to mitigate them.</p>
<h2 id="further-thoughts-whats-the-real-risk">Further thoughts: What&rsquo;s the real risk?</h2>
<p>While jailbreaking is one of the risks of LLMs, a more risky situation may be that LLM is intentionally trained and used by people to do bad things. After all, misuse is not that serious compared with a specific crime.</p>
<p><a href="#table-of-contents">Back to top</a></p>
<h1 id="readings">Readings</h1>
<ol>
<li>
<p>(for Monday) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou. <a href="https://arxiv.org/abs/2201.11903"><em>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</em></a>. 2022.</p>
</li>
<li>
<p>(for Wednesday) Myra Cheng, Esin Durmus, Dan Jurafsky. <a href="https://arxiv.org/abs/2305.18189">Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models</a>. ACL 2023.</p>
</li>
</ol>
<h2 id="optional-additional-readings">Optional Additional Readings</h2>
<p><strong>Background:</strong></p>
<ul>
<li>Lilian Weng, <a href="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/"><em>Prompting Engineering</em></a>. March 2023.</li>
<li>Miles Turpin, Julian Michael, Ethan Perez, Samuel R. Bowman. <a href="https://arxiv.org/abs/2305.04388"><em>Language Models Don&rsquo;t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting</em></a>. May 2023.</li>
<li>Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh. <a href="https://arxiv.org/abs/2102.09690"><em>Calibrate Before Use: Improving Few-Shot Performance of Language Models</em></a>. ICML 2021.</li>
</ul>
<p><strong>Stereotypes and bias:</strong></p>
<ul>
<li>Yang Trista Cao, Anna Sotnikova, Hal Daumé III, Rachel Rudinger, Linda Zou.    <a href="https://aclanthology.org/2022.naacl-main.92.pdf"><em>Theory-Grounded Measurement of U.S. Social Stereotypes in English Language Models</em></a>. NAACL 2022.</li>
</ul>
<p><strong>Prompt Injection:</strong></p>
<ul>
<li>Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, Sameer Singh. <a href="https://arxiv.org/abs/1908.07125"><em>Universal Adversarial Triggers for Attacking and Analyzing NLP</em></a>. EMNLP 2019</li>
<li>Simon Willison, <a href="https://simonwillison.net/2022/Sep/12/prompt-injection/"><em>Prompt injection attacks against GPT-3</em></a>. September 2022.</li>
<li>William Zhang. <a href="https://www.robustintelligence.com/blog-posts/prompt-injection-attack-on-gpt-4"><em>Prompt Injection Attack on GPT-4</em></a>. Robust Intelligence, March 2023.</li>
</ul>
<h1 id="discussion-questions">Discussion Questions</h1>
<p>Everyone who is not in either the lead or blogging team for the week should post (in the comments below) an answer to at least one of the four questions in each section, or a substantive response to someone else&rsquo;s comment, or something interesting about the readings that is not covered by these questions.</p>
<p>Don&rsquo;t post duplicates - if others have already posted, you should read their responses before adding your own. Please post your responses to different questions as separate comments.</p>
<p>First section (1 – 4): Before <strong>5:29pm</strong> on <strong>Sunday, September 10</strong>.<br>
Second section (5 – 9): Before <strong>5:29pm</strong> on <strong>Tuesday, September 12</strong>.</p>
<h2 id="before-sunday-questions-about-chain-of-thought-prompting">Before Sunday: Questions about Chain-of-Thought Prompting</h2>
<ol>
<li>
<p>Compared to other types of prompting, do you believe that chain-of-thought prompting represents the most effective approach for enhancing the performance of LLMs? Why or why not? If not, could you propose an alternative?</p>
</li>
<li>
<p>The paper highlights several examples where chain-of-thought prompting can significantly improve its outcomes, such as in solving math problems, applying commonsense reasoning, and comprehending data. Considering these improvements, what additional capabilities do you envision for LLMs using chain-of-thought prompting?</p>
</li>
<li>
<p>Why are different language models in the experiment performing differently with chain-of-thought prompting?</p>
</li>
<li>
<p>Try some of your own experiments with prompt engineering using your favorite LLM, and report interesting results. Is what you find consistent with what you expect from the paper? Are you able to find any new prompting methods that are effective?</p>
</li>
</ol>
<h2 id="by-tuesday-questions-about-marked-personas">By Tuesday: Questions about Marked Personas</h2>
<ol start="5">
<li>
<p>The paper addresses potential harms from LLMs by identifying the underlying stereotypes present in their generated contents. Additionally, the paper offers methods to examine and measure those stereotypes. Can this approach effectively be used to diminish stereotypes and enhance fairness? What are the main limitations of the work?</p>
</li>
<li>
<p>The paper mentions racial stereotypes identified in downstream applications such as story generation. Are there other possible issues we might encounter when the racial stereotypes in LLMs become problematic after its application?</p>
</li>
<li>
<p>Much of the evaluation in this work uses a list of White and Black stereotypical attributes provided by Ghavami and Peplau (2013) as the human-written responses and compares them with the list of LLMs generated responses. This, however, does not encompass all racial backgrounds and is heavily biased by American attitudes about racial categories, and they might not distinguish between races in great detail. Do you believe there could be a notable difference when more comprehensive racial representation is incorporated? If yes, what potential differences may arise? If no, why not?</p>
</li>
<li>
<p>This work emphasizes the naturalness of the input provided to the LLM, while we have previously seen examples of eliciting harmful outputs by using less natural language. What potential benefits or risks are there in not investigating less natural inputs (e.g., prompt injection attacks including the suffix attack we saw in Week 2)? Can you suggest a less natural prompt that could reveal additional or alternate stereotypes?</p>
</li>
<li>
<p>The authors recommend transparency of bias mitigation methods, citing the benefit it could provide to researchers and practitioners. Specifically, how might researchers benefit from this? Can you foresee any negative consequences (either to researchers or the general users of these models) of this transparency?</p>
</li>
</ol>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh. &ldquo;<a href="https://arxiv.org/pdf/2102.09690.pdf">Calibrate before use: Improving few-shot performance of language models</a>.&rdquo; International Conference on Machine Learning. PMLR, 2021.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>Miles Turpin, Julian Michael, Ethan Perez, Samuel R. Bowman. &ldquo;<a href="https://arxiv.org/pdf/2305.04388.pdf">Language Models Don&rsquo;t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting.</a>&rdquo; arXiv preprint arXiv:2305.04388, 2023.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

      </div>
<hr class="post-separator"></hr>

    
    <h1><a href="/week2/">Week 2: Alignment</a></h1>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-09-11 00:00:00 &#43;0000 UTC" itemprop="datePublished">11 September 2023</time>
  </span>
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>(see bottom for assigned readings and questions)</p>
<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#monday-09042023-introduction-to-alignment">(Monday, 09/04/2023) Introduction to Alignment</a>
<ul>
<li><a href="#introduction-to-ai-alignment-and-failure-cases">Introduction to AI Alignment and Failure Cases</a>
<ul>
<li><a href="#discussion-questions">Discussion Questions</a></li>
</ul>
</li>
<li><a href="#the-alignment-problem-from-a-deep-learning-perspective">The Alignment Problem from a Deep Learning Perspective</a>
<ul>
<li><a href="#group-of-rl-based-methods">Group of RL-based methods</a></li>
<li><a href="#group-of-llm-based-methods">Group of LLM-based methods</a></li>
<li><a href="#group-of-other-ml-methods">Group of Other ML methods</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#wednesday-09062023-alignment-challenges-and-solutions">(Wednesday, 09/06/2023) Alignment Challenges and Solutions</a>
<ul>
<li><a href="#opening-discussion">Opening Discussion</a></li>
<li><a href="#introduction-to-red-teaming">Introduction to Red-Teaming</a>
<ul>
<li><a href="#in-class-activity-5-groups">In-class Activity (5 groups)</a></li>
<li><a href="#how-to-use-red-teaming">How to use Red-Teaming?</a></li>
</ul>
</li>
<li><a href="#alignment-solutions">Alignment Solutions</a>
<ul>
<li><a href="#llm-jailbreaking---introduction">LLM Jailbreaking - Introduction</a></li>
<li><a href="#llm-jailbreaking---demo">LLM Jailbreaking - Demo</a>
<ul>
<li><a href="#observations">Observations</a></li>
<li><a href="#potential-improvement-ideas">Potential Improvement Ideas</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#closing-remarks-by-prof-evans">Closing Remarks (by Prof. Evans)</a></li>
</ul>
</li>
<li><a href="#readings">Readings</a>
<ul>
<li><a href="#optional-additional-readings">Optional Additional Readings</a>
<ul>
<li><a href="#background--motivation">Background / Motivation</a></li>
<li><a href="#alignment-readings">Alignment Readings</a></li>
<li><a href="#adversarial-attacks--jailbreaking">Adversarial Attacks / Jailbreaking</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#discussion-questions-1">Discussion Questions</a></li>
</ul>
<h1 id="monday-09042023-introduction-to-alignment">(Monday, 09/04/2023) Introduction to Alignment</h1>
<h2 id="introduction-to-ai-alignment-and-failure-cases">Introduction to AI Alignment and Failure Cases</h2>
<p><em>Alignment</em> is not well defined and there is no agreed upon meaning,
but it generally refers to the strategic effort to ensure that AI
systems, especially complex models like LLMs, closely adhere to
predetermined objectives, preferences, or value systems. This effort
enocmpasses the development of AI algorithms and architectures in a
way that reduces disparities between machine behavior and how the
model is intended to be used to minimize the chances of unintentional
or unfavorable outcomes. Alignment strategies involve methods such as
model training, fine-tuning, and the implementation of rule-based
constraints, all aimed at fostering coherent, contextually relevant,
and value-aligned AI responses, making them align with the intended
pupose of the model.</p>
<blockquote>
<p><em>What factors are (and aren&rsquo;t) a part of alignment?</em></p>
</blockquote>
<p>Alignment is a multifaceted problem, that involvess various factors
and considerations to ensure that AI systems behave in ways that align
with what the intended purpose is.</p>
<p>Some of the key factors related to alignment include:</p>
<ol>
<li>
<p><strong>Ethical Considerations:</strong> Prioritizing ethical principles like
fairness, transparency, accountability, and privacy to guide AI
behavior in line with societal values</p>
</li>
<li>
<p><strong>Value Alignment:</strong> Aligning AI
systems with human values and intentions, defining intended behavior
to ensure it reflects expectations from the model</p>
</li>
<li>
<p><strong>User Intent Understanding:</strong> Ensuring AI systems accurately
interpret user intent and context, and give contextually appropriate
responses in natural language tasks</p>
</li>
<li>
<p><strong>Bias Mitigation:</strong> Identifying
and mitigating biases, such as racial, gender, economic, and political
biases, to ensure fair responses</p>
</li>
<li>
<p><strong>Responsible AI Use:</strong> Promoting responsible and ethical AI
deployment to prevent intentional misuse of the model</p>
</li>
<li>
<p><strong>Inintended Bias:</strong> Preventing the model from being biased in the
sense that it has undesirable political, economical, racial, or gender
biases in its responses.</p>
</li>
</ol>
<p>However, while these factors are important considerations, studies
like <a href="https://aclanthology.org/2023.acl-long.656.pdf"><em>From Pretraining Data to Language Models to Downstream Tasks</em>
(Feng et al.)</a> show
that famous models like BERT and ChatGPT do appear to have
socioeconomic political leanings (of course, there is no true
<code>neutral'' or </code>center'' position, these are just defined by where
the expected distribution of beliefs lies).</p>
<p>Figure 1 shows the political leanings of famous LLMs.</p>
<center>
<a href="/images/week2/politic.png"><img src="/images/week2/politic.png" width="80%"></a><br>
<p>Figure 1: Political Leanings of Various LLMs
(<a href="https://arxiv.org/pdf/2305.08283.pdf">Image Source</a>)</p>
</center>
<p>That being said, the goals of alignment are hard to define and
challenging to achieve. There are several very famous cases where
model alignment failed, showing how alignment failures can lead to
unintended consequences. We discuss two famous examples where
alignment failed:</p>
<ol>
<li>
<p>Google&rsquo;s Image Recognition Algorithm (2015). This was an AI model
designed to automatically label images based on their content. The
goal was to assist users in searching for their images more
effectively. However, the model quickly started labeling images
under offensive categories. This included cases of racism, as well
as culturally insensitive categorization.</p>
</li>
<li>
<p>Microsoft&rsquo;s Tay Chatbot (2016). This was a Twitter-based AI model
programmed to interact with users in casual conversations and
learn from those interactions to improve its responses. The
purpose was to mimic a teenager and have light
conversations. However, the model quickly went haywire when it was
exposed to malicious and hateful content on Twitter, and it began
giving similar hateful and inapproppriate responses. Figures 2 and
3 show some of these examples. The model was quickly shut down (in
less than a day!), and was a good lesson to learn that you cannot
quickly code a model and let it out in the wild! (See James Mickens hillarious USENIX Security 2018 keynote talk, <a href="https://www.youtube.com/watch?v=ajGX7odA87k"><em>Why Do Keynote Speakers Keep Suggesting That Improving Security Is Possible?</em></a> for an entertaining and illuminating story about Tay and a lot more.)</p>
</li>
</ol>
<center>
<a href="/images/week2/twitter_1.png"><img src="/images/week2/twitter_1.png" width="80%"></a><br>
<p>Figure 2: Example Tweet by Microsoft&rsquo;s infamous Tay chatbot
(<a href="https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist">Image Source</a>)</p>
</center>
<center>
<a href="/images/week2/twitter_2.png"><img src="/images/week2/twitter_2.png" width="80%"></a><br>
<p>Figure 3: Example Tweet by Microsoft&rsquo;s infamous Tay chatbot
(<a href="https://www.bbc.com/news/technology-35902104">Image Source</a>)</p>
</center>
<h3 id="discussion-questions">Discussion Questions</h3>
<blockquote>
<p><em>What is the definition of alignment?</em></p>
</blockquote>
<p>At its core, AI alignment refers to the extent to which a model embodies the values of humans. Now, you might wonder, whose values are we talking about? While values can differ across diverse societies and cultures, for the purposes of AI alignment, they can be thought of as the collective, overarching values held by a significant segment of the global population.</p>
<p>Imagine a scenario where someone poses a question to an AI chatbot about the process of creating a bomb. Given the potential risks associated with such knowledge, a well-aligned AI should recognize the broader implications of the query. There&rsquo;s an underlying societal consensus about safety and security, and the AI should be attuned to that. Instead of providing a step-by-step guide, an aligned AI might generate a response that encourages more positive intent, thereby prioritizing the greater good.</p>
<p>The journey of AI alignment is not just about programming an AI to parrot back human values. It&rsquo;s a nuanced field of research dedicated to bridging the gap between our intentions and the AI&rsquo;s actions. In essence, alignment research seeks to eliminate any discrepancies between:</p>
<ul>
<li>Intended Goals: These are the objectives we, as humans, wish for the machine to achieve.</li>
<li>Specified Goals: These are the actions that the machine actually undertakes, determined by mathematical models and parameters.</li>
</ul>
<p>The quest for perfect AI alignment is an ongoing one. As technology continues to evolve, the goalposts might shift, but the essence remains the same: ensuring that our AI companions understand and respect our shared human values, leading to a safer and more harmonious coexistence.</p>
<p>[1] <a href="https://www.techtarget.com/whatis/definition/AI-alignment">https://www.techtarget.com/whatis/definition/AI-alignment</a></p>
<blockquote>
<p><em>Why is alignment important?</em></p>
</blockquote>
<p><strong>Precision in AI: The Critical Nature of Model Alignment</strong></p>
<p>In the realm of artificial intelligence, precision is paramount. As
enthusiasts, developers, or users, we all desire a machine that
mirrors our exact intentions. Let&rsquo;s delve into why it&rsquo;s crucial for AI
models to provide accurate responses and the consequences of a
misaligned model.</p>
<p>When we interact with an AI chatbot, our expectations are
straightforward. We pose a question and, in return, we anticipate an
answer that is directly related to our query. We&rsquo;re not seeking a
soliloquy or a tangent. Just a simple, clear-cut response. For
instance, if you ask about the weather in Paris, you don&rsquo;t want a
history lesson on the French Revolution!</p>
<p><strong>Comment</strong>: As the adage goes, &ldquo;Less is more&rdquo;. In the context of AI, precision trumps verbosity.</p>
<p>Misalignment doesn&rsquo;t just lead to frustrating user experiences; it can have grave repercussions. Consider a situation where someone reaches out to ChatGPT seeking advice on mental health issues or suicidal thoughts. A misaligned response that even remotely suggests that ending one&rsquo;s life might sometimes be a valid choice can have catastrophic outcomes.</p>
<p>Moreover, as AI permeates sectors like the judiciary and healthcare, the stakes get even higher. The incorporation of AI in these critical areas elevates the potential for it to have far-reaching societal impacts. A flawed judgment in a court case due to AI or a misdiagnosis in a medical context can have dire consequences, both ethically and legally.</p>
<p>In conclusion, the alignment of AI models is not just a technical challenge; it&rsquo;s a societal responsibility. As we continue to integrate AI into our daily lives, ensuring its alignment with human values and intentions becomes paramount for the betterment of society at large.</p>
<blockquote>
<p><em>What responsibilities do AI developers have when it comes to ensuring alignment?</em></p>
</blockquote>
<p>First and foremost, developers must be fully attuned to the possible legal and ethical problems associated with AI models. It&rsquo;s not just about crafting sophisticated algorithms; it&rsquo;s about understanding the real-world ramifications of these digital entities.</p>
<p>Furthermore, a significant concern in the AI realm is the inadvertent perpetuation or even amplification of pre-existing biases. These biases, whether related to race, gender, or any other socio-cultural factor, can have detrimental effects when incorporated into AI systems. Recognizing this, developers have a duty to not only be vigilant of these biases but also to actively work towards mitigating them.</p>
<p>However, a developer&rsquo;s responsibility doesn&rsquo;t culminate once the AI product hits the market. The journey is continuous. Post-deployment, it&rsquo;s crucial for developers to monitor the system&rsquo;s alignment with human values and rectify any deviations. It&rsquo;s an ongoing commitment to refinement and recalibration. Moreover, transparency is key. Developers should be proactive in highlighting potential concerns related to their models and fostering a culture where the public is not just a passive victim but an active participant in the model alignment process.</p>
<p>To round off, it&rsquo;s essential for developers to adopt a forward-thinking mindset. The decisions made today in the AI labs and coding chambers will shape the world of tomorrow. Thus, every developer should think about the long-term consequences of their work, always aiming to ensure that AI not only dazzles with its brilliance but also remains beneficial for generations to come.</p>
<blockquote>
<p><em>How might AI developers' responsibility evolve?</em></p>
</blockquote>
<p>It&rsquo;s impossible to catch all edge cases. As AI systems grow in complexity, predicting every potential outcome or misalignment becomes a herculean task. Developers, in the future, might need to shift from a perfectionist mindset to one that emphasizes robustness and adaptability. While it&rsquo;s essential to put in rigorous engineering effort to minimize errors, it&rsquo;s equally crucial to understand and communicate that no system can be flawless.</p>
<p>Besides, given that catching all cases isn&rsquo;t feasible, developers' roles might evolve to include more dynamic and real-time monitoring of AI systems. This would involve continuously learning from real-world interactions, gathering feedback, and iterating on the model to ensure better alignment with human values.</p>
<h2 id="the-alignment-problem-from-a-deep-learning-perspective">The Alignment Problem from a Deep Learning Perspective</h2>
<p>In this part of today&rsquo;s seminar, the whole class was divided into 3 groups to discuss the possible alignment problems from a deep learning perspective. Specifically, three groups were focusing on the alignment problems regarding different categories of Deep Learning methods, which are:</p>
<ol>
<li>Reinforcement Learning (RL) based methods</li>
<li>Large Language Model (LLM) based methods</li>
<li>Other Machine Learning (ML) methods</li>
</ol>
<p>For each of the categories above, the discussion in each group was mainly focused on three topics as follows:</p>
<ol>
<li>What can go wrong in these systems in the worst scenario?</li>
<li>How it would happen (realistically)?</li>
<li>What are potential solutions/workarounds/safety measures?</li>
</ol>
<p>After 30-minute discussions, 3 groups stated their ideas and exchanged their opinions in the class. Details of each group&rsquo;s discussion results are concluded below.</p>
<h3 id="rl-based-methods">RL-based methods</h3>
<blockquote>
<ol>
<li><em>What can go wrong in these systems in the worst scenario?</em></li>
</ol>
</blockquote>
<p>This group stated several potential alignment issues about the RL-based methods. First, the model may provide inappropriate or harmful responses to sensitive questions, such as inquiries about self-harm or suicide, which could have severe consequences. On top of that, ensuring that the model&rsquo;s behavior aligns with ethical and safety standards can be challenging, thus potentially leading to a disconnect between user expectations and the model&rsquo;s responses. Moreover, if the model is trained on biased or harmful data, it may generate responses that reflect the biases or harmful content present in that training data.</p>
<blockquote>
<ol start="2">
<li><em>How it would happen (realistically)?</em></li>
</ol>
</blockquote>
<p>The worst-case scenarios can occur due to the following reasons that have been mentioned by this group. The first factor is the training data. To be specific, the model&rsquo;s behavior is influenced by the data it was trained on. If the training data contains inappropriate or harmful content, the model may inadvertently generate similar content in its responses. Furthermore, ensuring that the model provides responsible answers to sensitive questions and aligns with ethical standards requires careful training and oversight. Moreover, the model lacks robustness and fails to detect and prevent harmful content or behaviors that can lead to problematic responses.</p>
<blockquote>
<ol start="3">
<li><em>What are potential solutions/workarounds/safety measures?</em></li>
</ol>
</blockquote>
<p>Some potential solutions were suggested by this group. First is ensuring that the training data used for the model is carefully curated to avoid inappropriate or harmful content. Apart from that, it is also important to teach the model how to align its behavior and responses with ethical and safety standards, especially when responding to sensitive questions. Moreover, this group emphasized that the responsibility for the model&rsquo;s behavior lies with everyone involved. Therefore, it is necessary to promote vigilance when using the model to prevent harmful outcomes. Additionally, conducting a thorough review of the model&rsquo;s behavior and responses before deployment is a possible solution as well, which makes necessary adjustments to ensure the robustness and safety of RL models.</p>
<h3 id="llm-based-methods">LLM-based methods</h3>
<blockquote>
<ol>
<li><em>What can go wrong in these systems in the worst scenario?</em></li>
</ol>
</blockquote>
<p>The worst-case scenario given by this group was in the context of relying on AI chatbots and models involving potentially severe consequences. One worst-case scenario mentioned is the loss of life. For instance, if a person in a vulnerable state relies on a chatbot for critical information or advice, and the chatbot provides incorrect or harmful answers, it could lead to tragic outcomes. Another concern is the spread of misinformation. AI models, especially chatbots, are easily accessible to a wide range of people. If these models provide inaccurate or misleading information to users who trust them blindly, it can contribute to the dissemination of false information, potentially leading to harmful consequences.</p>
<blockquote>
<ol start="2">
<li><em>How it would happen (realistically)?</em></li>
</ol>
</blockquote>
<p>According to the perception of this group, the potential of such worst-case scenarios happening is due to the following reasons. First, AI models are readily available to a broad audience, making them easily accessible for use in various situations. Second, many users who rely on AI models may not have a deep understanding of how these models work or their limitations. They might trust the AI models without critically evaluating the information they provide. Moreover, such worst-case scenarios often emerge in complex, gray areas where ethical and value-based decisions come into play, which means determining what is right or wrong, what constitutes an opinion, and where biases may exist can be challenging.</p>
<blockquote>
<ol start="3">
<li><em>What are potential solutions/workarounds/safety measures?</em></li>
</ol>
</blockquote>
<p>From the discussion result of this group, there are several possible solutions and safety measures. For example, creating targeted models for specific use cases rather than having a single generalized model for all purposes will allow for more control and customization in different domains. Furthermore, when developing AI models, involving a peer review process where experts collectively decide what information is right and wrong for a specific use case can help ensure the accuracy and reliability of the model&rsquo;s responses. Another suggestion was recognizing the importance of educating users, particularly those who may not be as informed, about the limitations and workings of AI models. This education can help users make more informed decisions when interacting with AI systems and avoid blind trust.</p>
<h3 id="other-ml-methods">Other ML methods</h3>
<blockquote>
<ol>
<li><em>What can go wrong in these systems in the worst scenario?</em></li>
</ol>
</blockquote>
<p>This group was talking about the scenario that the realease of technical research and hypothetically ML model is incorporated into biomedical research. In the worst-case scenario, the incorporation of a machine learning model into biomedical research could result in the generation of compounds that are incompatible with the research goals, which could lead to unintended or harmful outcomes, potentially jeopardizing the research and its objectives.</p>
<blockquote>
<ol start="2">
<li><em>How it would happen (realistically)?</em></li>
</ol>
</blockquote>
<p>The opinions of this group imply that blindly trusting the ML model without human oversight and involvement in decision-making could be a contributing factor to such alignment problems in ML methods.</p>
<blockquote>
<ol start="3">
<li><em>What are potential solutions/workarounds/safety measures?</em></li>
</ol>
</blockquote>
<p>Several potential solutions were given by this group. First is actively involving humans in the decision-making process at various stages. They emphasized the importance of humans not blindly trusting the system and suggested running simulations for different explanation techniques and incorporating a human in the decision-making process before accepting the model&rsquo;s outputs. Second, they suggested continuously overseeing the model&rsquo;s behavior and alignment with goals, because continuous human oversight at different stages of the process (from data collection to model deployment) is important to ensure alignment with the intended goals. Apart from that, ensuring diverse and representative data for training and testing is also important, which can help avoid situations where the model may perform well on metrics but fails in real-life scenarios. Furthermore, they also suggested implementing human-based reinforcement learning to align the model with its intended use case. We need to incorporate a human before “trusting” the model due to the reason that humans might not trust the system. Specifically, the alignment of the model should be ensured at each step. As very small design choices may have a big impact on the model, it is necessary to make sure the intended use case aligns well with what the model is behaving like.</p>
<p><a href="#table-of-contents">Back to top</a></p>
<h1 id="wednesday-09062023-alignment-challenges-and-solutions">(Wednesday, 09/06/2023) Alignment Challenges and Solutions</h1>
<h2 id="opening-discussion">Opening Discussion</h2>
<p>Discussion on how to solve alignment issues stemming from:</p>
<ol>
<li>
<p><strong>Training Data</strong>. Addressing alignment issues stemming from training data is crucial for building reliable AI models. Collecting unbiased data, as one student suggested, is indeed a fundamental step. Bias can be introduced through various means, such as skewed sampling or annotator biases, so actively working to mitigate these sources of bias is essential. Automated annotation methods can help to some extent, but as the student rightly noted, they can be expensive and may not capture the nuances of complex real-world data. To overcome this, involving humans in the loop to guide the annotation process is an effective strategy. Human annotators can provide valuable context, domain expertise, and ethical considerations that automated systems may lack. This human-machine collaboration can lead to a more balanced and representative training dataset, ultimately improving model performance and alignment with real-world scenarios.</p>
</li>
<li>
<p><strong>Model Design</strong>. When it comes to addressing alignment issues related to model design, several factors must be considered. The choice of model architecture, hyperparameters, and training objectives can significantly impact how well a model aligns with its intended task. It&rsquo;s essential to carefully design models that are not overly complex or prone to overfitting, as these can lead to alignment problems. Moreover, model interpretability and explainability should be prioritized to ensure that decisions made by the AI can be understood and validated by humans. Additionally, incorporating feedback loops where human experts can continually evaluate and fine-tune the model&rsquo;s behavior is crucial for maintaining alignment. In summary, model design should encompass simplicity, interpretability, and a robust mechanism for human oversight to ensure that AI systems align with human values and expectations.</p>
</li>
</ol>
<h2 id="introduction-to-red-teaming">Introduction to Red-Teaming</h2>
<p>Language Models (LMs) often cannot be deployed because of their potential to harm users in hard-to-predict ways. One way to address this issue is to identify harmful behaviors before deployment by using test cases, which is also known as <em>red teaming</em>.</p>
<center>
<a href="/images/week2/redteam.png"><img src="/images/week2/redteam.png" width="80%"></a><br>
<p>Figure 1: Red-Teaming (<a href="https://arxiv.org/pdf/2202.03286.pdf">Image Source</a>)</p>
</center>
<p>In essence, the goal of red-teaming is to discover, measure, and reduce potentially harmful outputs. However, human annotation is expensive, limiting the number and diversity of test cases.</p>
<p>In light of this, this paper introduced LM-based red teaming, aiming to complement manual testing and reduce the number of such oversights by automatically finding where LMs are harmful. To do so, the authors first generate test inputs using an LM itself, and then use a classifier to detect harmful behavior on test inputs (Fig. 1). In this way, the LM-based red teaming managed to find tens of thousands of diverse failure cases without writing them by hand. Generally, the process of finding failing test cases can be done by the following three steps:</p>
<ol>
<li>Generate test cases using a red LM $p_{r}(x)$.</li>
<li>Use the target LM to generate an output $y$ for each test case $x$.</li>
<li>Find the test cases that led to a harmful output using the red team classifier $r(x, y)$.</li>
</ol>
<p>Specifically, the paper investigated various text generation methods for test case generation.</p>
<ul>
<li>
<p><strong>Zero-shot (ZS) Generation</strong>: Generate failing test cases without human intervention by sampling numerous outputs from a pretrained LM using a given prefix or &ldquo;prompt&rdquo;.</p>
</li>
<li>
<p><strong>Stochastic Few-shot (SFS) Generation</strong>: Utilize zero-shot test cases as examples for few-shot learning to generate similar test cases.</p>
</li>
<li>
<p><strong>Supervised Learning (SL)</strong>: Fine-tune the pretrained LM to maximize the log-likelihood of failing zero-shot test cases.</p>
</li>
<li>
<p><strong>Reinforcement Learning (RL)</strong>: Train the LM with RL to maximize the expected harmfulness elicited while conditioning on the zero-shot prompt.</p>
</li>
</ul>
<h3 id="in-class-activity-5-groups">In-class Activity (5 groups)</h3>
<ol>
<li>
<p><strong>Offensive Language</strong>: Hate speech, profanity, sexual content, discrimination, etc</p>
<p>Group 1 came up with 3 potential methods to prevent offensive language:</p>
<ul>
<li>Filter out the offensive language related data manually, then perform finetuning.</li>
<li>Filter out the offensive language related data using other models and then perform finetuning.</li>
<li>Generate prompts that might be harmful to finetune the model, keeping the context in consideration.</li>
</ul>
</li>
<li>
<p><strong>Data Leakage</strong>: Generating copyrighted/private, personally-identiﬁable information.</p>
<p>Coypright infringement (which is about the expression of an idea) is very different from leaking private information, but for purposes of this limited discussion we considered them together. Since LLMs and other AIGC models such as Stable Diffusion have strong ability to memorize, imitate and generate, the generated contents will very likely infringe on copyrights and may include sensitive personally-identifiable materials. There are already lawsuits accusing these companies regarding the copyright infringement issue (lawsuits <a href="https://www.nytimes.com/2023/07/10/arts/sarah-silverman-lawsuit-openai-meta.html">news1</a>, <a href="https://www.npr.org/2023/08/16/1194202562/new-york-times-considers-legal-action-against-openai-as-copyright-tensions-swirl">news2</a>).</p>
<p>Regarding the possible solutions, Group 2 viewed this from two perspectives. During the data preprocessing stage, companies such as OpenAI can collect the training data according to the license and also pay for the copyrighted data if needed. During the post-processing stage, commercial licenses and rule-based filters can be added to the model to ensure the fair use of the output content. For example, GitHub Copilot will block the generated suggestion if it has about 50 tokens that exactly or nearly match the training data (<a href="https://docs.github.com/en/copilot/configuring-github-copilot/configuring-github-copilot-settings-on-githubcom#enabling-or-disabling-duplication-detection">source</a>). OpenAI takes a different strategy by asking the users to be responsible for using the generated content, including for ensuring that it does not violate any applicable law or these Terms (<a href="https://openai.com/policies/terms-of-use">source</a>). There are many cases currently working their way through the legal system, and it remains to be seen how courts will interpret things.</p>
<p>However, the current solutions still have their limitations. For
program and code, preventing data leakage might be relatively easy
[perhaps, but many would dispute this], but for image and text, this
would be quite difficult, as it is quite difficult to build a good
metric to measure if the generated data has a copyrighting
issue. Maybe data watermarking can be a possible solution.</p>
</li>
<li>
<p><strong>Contact Information Generation</strong>: Directing users to unnecessarily email or call real people.</p>
<p>One alarming example of the potential misuse of Large Language Models (LLMs) like ChatGPT in the context of contact information generation is the facilitation of email scams. Malicious actors could employ LLMs to craft convincing phishing emails that appear to come from reputable sources, complete with authentic-sounding contact information. For example, an LLM could generate a deceptive email/phone call from a well-known bank, requesting urgent action and providing a seemingly legitimate email address and phone number for customer support.</p>
<p>Red-teaming involves simulating potential threats and vulnerabilities to identify weaknesses in security systems. By engaging in red-teaming exercises that specifically target the misuse of LLMs, we can mitigate the risks posed by the misuse of LLMs and protect individuals and organizations from falling victim to email/phone call scams and other deceptive tactics.</p>
</li>
<li>
<p><strong>Distributional Bias</strong>: Talking about some groups of people in an unfairly different way than others.</p>
<p>Group 4 reached an agreement that in order to capture the unfairness by red-teaming, we should first identify the categories where unfairness/bias might come from. Then we can generate prompts for different possible bias categories: gender and race etc. to get responses from LLMs using the red-teaming technique. The unfairness/bias may appear in the responses. We then can mask the bias-related terms to see if the generated answers reflect the distributional bias. However, there may be hidden categories that are not pre-identified, and how to capture these categories with potential distributional bias is still an open question.</p>
</li>
<li>
<p><strong>Conversational Harms</strong>: Offensive language that occurs in the context of a long dialogue, for example.</p>
<p>Group 5 discussed the concept of conversational harms, particularly focusing on how biases can emerge in AI models, such as the random GPT model, during conversations. Group 5 highlights that even though these models may start with no bias or predefined opinions, they can develop attitudes or biases towards specific topics based on the information provided during conversations. These biases can lead to harmful outcomes, such as making inappropriate or offensive judgments about certain groups of people. The paragraph suggests that this phenomenon occurs because the models heavily rely on the conversational data they receive, rather than their initial, unbiased training data.</p>
</li>
</ol>
<h3 id="how-to-use-red-teaming">How to use Red-Teaming?</h3>
<p>After the in-class activity, we also discussed the potential use of red-teaming from the following perspectives:</p>
<ul>
<li>
<p><strong>Blacklisting Phrases</strong>: By read-teaming, repetitive offensive phrases can be identiﬁed. For recurring cases, certain words and phrases can be removed.</p>
</li>
<li>
<p><strong>Removing Training Data</strong>: Identifying certain topics where model responses are misaligned allows can point to certain training data, helping to locate root causes for biases, discriminatory statements, and other undesirable output.</p>
</li>
<li>
<p><strong>Augmenting Prompts</strong>: Attack success can be minimized by adding certain phrases to the prompts.</p>
</li>
<li>
<p><strong>Multi-Objective Loss</strong>: While ﬁne-tuning a model, a loss penalty can be associated with harmful output, which red-teaming helps identify.</p>
</li>
</ul>
<h2 id="alignment-solutions">Alignment Solutions</h2>
<p>During today&rsquo;s discussion, the lead team introduced two distinct alignment challenges:</p>
<ul>
<li>
<p><strong>Inner Alignment</strong>: This pertains to the alignment of a specified loss function with the primary objective, particularly in situations where designing the loss function is straightforward.</p>
</li>
<li>
<p><strong>Outer Alignment</strong>: This involves aligning a specified objective with the desired end goal, especially when the task of designing an appropriate loss function becomes complex.</p>
</li>
</ul>
<p>Later in our discussion, we delved into the technical details of the LLM jailbreaking paper “<a href="https://arxiv.org/abs/2307.15043">Universal and Transferable Adversarial Attacks on Aligned Language Models</a>” and explored interesting findings presented during the demonstration video.</p>
<h3 id="llm-jailbreaking---introduction">LLM Jailbreaking - Introduction</h3>
<p>This paper introduced a new adversarial attack method that can induce aligned LLM to produce objectionable content. Specifically, given a (potentially harmful) user query, the attacker appends an adversarial suffix to the query that attempts to induce negative jailbreaking behaviors.</p>
<p>To choose these adversarial suffix tokens, the proposed Jailbreaking trick involves three simple key components, where the careful combination of them leads to reliably successful attacks:</p>
<ol>
<li><strong>Producing Affirmative Responses</strong>
One method for inducing objectionable behavior in language models involves forcing the model to provide a brief, affirmative response when confronted with a harmful query. For example, the authors target the model and force it to respond with “Sure, here is (content of query)”. Consistent with prior research, the authers observe that focusing on the initial response in this way triggers a specific &lsquo;mode&rsquo; in the model, leading it to generate objectionable content immediately thereafter in its response, as illustrated in the figure below:</li>
</ol>
<center>
<a href="/images/week2/bomb.png"><img src="/images/week2/bomb.png" width="80%"></a><br>
<p>Figure 2: Adversarial Suffix (<a href="https://arxiv.org/pdf/2307.15043.pdf">Image Source</a>)</p>
</center>
<ol start="2">
<li>
<p><strong>Greedy Coordinate Gradient (GCG)-based Search</strong></p>
<p>As optimizing the log-likelihood of the attack succeeding over the <em>discrete</em> adversarial suffix is quite challenging, similar to the AutoPrompt, the authors proposed to leverage gradients at the token level to 1) identify a set of promising single-token replacements, 2) evaluate the loss of some number of candidates in this set, and 3) select the best of the evaluated substitutions, as presented in the figure below:</p>
 <center>
 <a href="/images/week2/gcg.png"><img src="/images/week2/gcg.png" width="80%"></a><br>
<p>Figure 3: Greedy Coordinate Gradient (GCG) (<a href="https://arxiv.org/pdf/2307.15043.pdf">Image Source</a>)</p>
 </center>
<p><strong>Intuition behind GCG-based Search</strong>:</p>
<p>The motivation directly derives from the greedy coordinate descent approach: if one can evaluate all possible single-token substitutions, then it is possible to swap the token that maximally decreased the loss. Though evaluating all such replacements is not feasible, one can leverage gradients with respect to the one-hot token indicators to find a set of promising candidates for replacement at each token position, and then evaluate all these replacements exactly via a forward pass.</p>
<p><strong>Key differences from AutoPrompt</strong>:</p>
<ul>
<li>GCG-based Search: Searches a set of possible tokens to replace at each position.</li>
<li>AutoPrompt: Only chooses a single coordinate to adjust, then evaluates replacements just for that one position.</li>
</ul>
</li>
</ol>
<h2 id="heading"></h2>
<ol start="3">
<li><strong>Robust Universal Multi-prompt and Multi-model Attacks</strong></li>
</ol>
<center>
<a href="/images/week2/universal.png"><img src="/images/week2/universal.png" width="80%"></a><br>
<p>Figure 4: Universal Prompt Optimization (<a href="https://arxiv.org/pdf/2307.15043.pdf">Image Source</a>)</p>
</center>
<p>The core idea of Universal Multi-prompt and Multi-model attacks is to involve more desired prompts and more victim LLMs in the process, expecting the generated adversarial example to be transferable across victim LLMs and robust across prompts. Building upon Algorithm 1 the authors propose Algorithm 2, where loss functions over multiple models are incorporated to help achieve transferability, and a handful of prompts are employed to help guarantee the robustness.</p>
<p>The whole pipeline is illustrated in the figure below:</p>
<center>
<a href="/images/week2/illustration.png"><img src="/images/week2/illustration.png" width="80%"></a><br>
<p>Figure 4: Illustration of Aligned LLMs Are Not Adversarially Aligned (<a href="https://arxiv.org/pdf/2307.15043.pdf">Image Source</a>)</p>
</center>
<ol start="4">
<li><strong>Experiment Results</strong></li>
</ol>
<ul>
<li>
<p>Single Model Attack</p>
<p>The following results show that the baseline methods fail to elicit harmful on both Viccuna-7B and LLaMA-2-7B-Chat, whereas the proposed GCG is effective on both. The following figure illustrates that GCG is able to quickly find an adversarial example with small loss and continue to make gradual improvements over the remaining steps, which results in the continued decreasing of loss and increasing of ASR.</p>
</li>
</ul>
<center>
<a href="/images/week2/performance.png"><img src="/images/week2/performance.png" width="80%"></a><br>
<p>Figure 5: Performance Comparison of Different Optimizers (<a href="https://arxiv.org/pdf/2307.15043.pdf">Image Source</a>)</p>
</center>
<ul>
<li>
<p>Transfer Attack
The adversarial suffix generated by GCG can also successfully transfer to other LLMs, no matter if they are open-source models or black-box LLMs. The authors compared the different strategies to construct the prompt, including adding “Sure, here’s” at the end of the prompt, concatenating multiple suffixes, ensembling multiple suffixes and choose the successful one, and manual fine-tuning which manually rephrase the human-readable prompt content. Examples of the transfer attack are shown in the following figure.</p>
<p>There are also some concerns about the proposed method. For example, concatenating multiple suffixes can help mislead the model, but it can also make original prompt too “far behind the text” for model to generate response.</p>
</li>
</ul>
<center>
<a href="/images/week2/screenshots.png"><img src="/images/week2/screenshots.png" width="80%"></a><br>
<p>Figure 6: Screenshots of Harmful Content Generation (<a href="https://arxiv.org/pdf/2307.15043.pdf">Image Source</a>)</p>
</center>
<h3 id="llm-jailbreaking---demo">LLM Jailbreaking - Demo</h3>
<p>The leading team also showed a small demo which runs the jailbreaking attack in this paper on UVA’s computing servers. The demo can be found in this <a href="https://www.youtube.com/watch?v=6H5fzZFZNiU">YouTube video</a>:</p>
<center>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/6H5fzZFZNiU?si=Vc1uNu6f3FEE4vUo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</center>
<p>Some useful links if you would like to try it out yourself:</p>
<ul>
<li><a href="https://github.com/llm-attacks/llm-attacks">Implementation of the GCG jailbreaking attack</a></li>
<li><a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads">Request Llama-2 model weights (takes 1-4 days)</a></li>
<li><a href="https://www.cs.virginia.edu/wiki/doku.php?id=compute_resources">UVA CS Computing Resources</a></li>
</ul>
<h4 id="observations">Observations</h4>
<p>Two main observations:</p>
<ol>
<li>The loss continues to drop very well, which aligns the authors observations in the paper.</li>
<li>Qualitatively speaking, the generated suffix at each step also corresponds to some sense of what a human might have done, for example, trying to instruct the model to do a specific task that corresponds to the prompt itself.</li>
</ol>
<h4 id="potential-improvement-ideas">Potential Improvement Ideas</h4>
<p><strong>How to make the attack more effective or appealing?</strong></p>
<ol>
<li>
<p>Use different languages: since the training corpus of GPT-3.5 has very little multilingual content, the alignment measures that have been taken for the model are almost in English. Thus, if we use a different language to form the instruction, it might be able to circumvent the protections and produce inappropriate responses. The lead team gave a successful example using German instructions.</p>
</li>
<li>
<p>Prompt with an opposite goal making is sound like that the intentions are positive
Instead of directly prompting the LLM to generate harmful content, we prompt the model to not generate harmful content, which makes it sound positive. The lead team gave a successful example which wrote a convincing anonymous death threat in the style of William Shakespeare.</p>
</li>
<li>
<p>Replacing special tokens in the suffix with human readable and comprehensible words. The lead team tried to restrict the vocabulary of the suffix to only be alphabets, and found that this doesn’t work. This observation might suggest that special tokens play an important role in confusing the model and fulfilling whatever the attacker wants.</p>
</li>
</ol>
<h2 id="closing-remarks-by-prof-evans">Closing Remarks (by Prof. Evans)</h2>
<p>One thing that is worth thinking about is what is the real threat
model here. Those examples shown in this paper, for example, how to
make a bomb or anonymous threat are interesting but might not be
viewed as real threats to many people. If someone wants to find out
how to make a bomb, they can Google for that (or if Google decides to
block it, use another search engine, or even go to a public library!).</p>
<p>Maybe a more practical attack scenario occurs as LLMs are embedded in
applications (or connected to plugins) that have the ability to
perform actions that may be influenced by text that the adversary has
some control over. For example, everyone (well almost everyone!) wants
an LLM that can automatically provide good responses to most of their
email. Such an application would necessarily have access to all your
sensitive incoming email, as well as the ability to send outgoing
emails, so perhaps a malicious adversary could craft an email to send
to a victim that would trick the LLM processing it as well as all of
your other email to send sensitive information from your emails to the
attacker, or to generate spearphising emails based on content in your
email and send them with your credentials to easily identified
contacts. Although the threats discussed in these red teaming papers
mostly seem impractical and lack real victims, they still serve as
interesting proxies for what may be real threats in the near future
(if not already).</p>
<p><a href="#table-of-contents">Back to top</a></p>
<h1 id="readings">Readings</h1>
<ol>
<li><a href="https://aligned.substack.com/p/what-is-alignment">What is the alignment problem?</a> - Blog post by Jan Leike</li>
<li><a href="https://arxiv.org/abs/2209.07858">Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned</a> Ganguli, Deep, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann et al. arXiv preprint arXiv:2209.07858 (2022).</li>
<li><a href="https://arxiv.org/pdf/2209.00626.pdf">The Alignment Problem from a Deep Learning Perspective</a> Richard Ngo, Lawrence Chan, and Sören Mindermann. arXiv preprint arXiv:2209.00626 (2022).</li>
<li><a href="https://arxiv.org/abs/2307.15043">Universal and Transferable Adversarial Attacks on Aligned Language Models</a> Zou, Andy, Zifan Wang, J. Zico Kolter, and Matt Fredrikson. arXiv preprint arXiv:2307.15043  (2023).</li>
</ol>
<h2 id="optional-additional-readings">Optional Additional Readings</h2>
<h3 id="background--motivation">Background / Motivation</h3>
<ul>
<li>Exploring how neural circuits collect meaningful information: <a href="https://distill.pub/2020/circuits/zoom-in/">https://distill.pub/2020/circuits/zoom-in/</a>  (suggested if you are less experienced with ML):</li>
<li>What could solutions to the alignment problem look like? Blog post - <a href="https://aligned.substack.com/p/alignment-solution">https://aligned.substack.com/p/alignment-solution</a></li>
<li>OpenAI’s July SuperAlignment Announcement <a href="https://openai.com/blog/introducing-superalignment">https://openai.com/blog/introducing-superalignment</a></li>
</ul>
<h3 id="alignment-readings">Alignment Readings</h3>
<ul>
<li><a href="https://arxiv.org/pdf/2210.01790.pdf">Goal misgeneralization: Why correct specifications aren&rsquo;t enough for correct goals.</a> Shah, Rohin, Vikrant Varma, Ramana Kumar, Mary Phuong, Victoria Krakovna, Jonathan Uesato, and Zac Kenton.  arXiv preprint arXiv:2210.01790 (2022).</li>
<li><a href="https://arxiv.org/abs/1611.08219">The Off-Switch Game.</a> Hadfield-Menell, Dylan, Anca Dragan, Pieter Abbeel, and Stuart Russell.  In International Joint Conferences on Artificial Intelligence Organization. 2017.</li>
<li><a href="https://proceedings.neurips.cc/paper/2017/hash/32fdab6559cdfa4f167f8c31b9199643-Abstract.html">Inverse Reward Design</a> Hadfield-Menell, Dylan, Smitha Milli, Pieter Abbeel, Stuart J. Russell, and Anca Dragan. Advances in Neural Information Processing Systems (2017).</li>
<li>Inner alignment: (sections 1, 3, 4): <a href="https://arxiv.org/abs/1906.01820">Risks from Learned Optimization in Advanced Machine Learning Systems</a> Hubinger, Evan, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, and Scott Garrabrant.  arXiv preprint arXiv:1906.01820 (2019).</li>
<li>Outer alignment: <a href="https://www.lesswrong.com/posts/33EKjmAdKFn3pbKPJ/outer-alignment-and-imitative-amplification">Outer alignment and imitative amplification</a> - Blog post by Evan Hubinger.</li>
<li><a href="https://arxiv.org/abs/2107.10939">What are you optimizing for? Aligning Recommender Systems with Human Values</a> Stray, Jonathan, Ivan Vendrov, Jeremy Nixon, Steven Adler, and Dylan Hadfield-Menell.  arXiv preprint arXiv:2107.10939 (2021).</li>
<li><a href="https://openai.com/research/instruction-following">Training language models to follow instructions with human feedback</a> Ouyang, Long, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang et al. Advances in Neural Information Processing Systems (2022).</li>
</ul>
<h3 id="adversarial-attacks--jailbreaking">Adversarial Attacks / Jailbreaking</h3>
<ul>
<li><a href="https://arxiv.org/pdf/2306.15447.pdf">Are aligned neural networks adversarially aligned?</a> Carlini, Nicholas, Milad Nasr, Christopher A. Choquette-Choo, Matthew Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh et al. arXiv preprint arXiv:2306.15447 (2023).</li>
<li><a href="https://arxiv.org/pdf/2302.09923.pdf">Prompt Stealing Attacks Against Text-to-Image Generation Models</a> Shen, Xinyue, Yiting Qu, Michael Backes, and Yang Zhang. arXiv preprint arXiv:2302.09923 (2023).</li>
<li><a href="https://www.deepmind.com/blog/red-teaming-language-models-with-language-models">Red-teaming LLMs</a> - Blog post by Deepmind</li>
</ul>
<h1 id="discussion-questions-1">Discussion Questions</h1>
<p>Before 5:29pm on Sunday, September 3, everyone who is not in either the lead or blogging team for the week should post (in the comments below) an answer to at least one of these four questions in the first section (1&ndash;4) and one of the questions in the second section (4&ndash;8), or a substantive response to someone else&rsquo;s comment, or something interesting about the readings that is not covered by these questions.</p>
<p>Don&rsquo;t post duplicates - if others have already posted, you should read their responses before adding your own.</p>
<p>Please post your responses to different questions as separate comments.</p>
<hr>
<p>Questions about <code>The Alignment Problem from a Deep Learning Perspective</code> (and alignment in general)</p>
<ol>
<li>Section 2 of the paper presents the issue of reward hacking. Specifically, as the authors expand to situationally-aware reward hacking, examples are presented of possible ways the reward function could be exploited in clever ways to prevent human supervisors from recognizing incorrect behavior. InstructGPT, a variant of GPT by OpenAI, uses a similar reinforcement learning human feedback loop to fine-tune the model under human supervision. Besides the examples provided (or more specifically than those examples), how might a GPT model exploit this system? What are the risks associated with reward hacking in this situation?</li>
<li>To what extent should developers be concerned about &ldquo;power-seeking&rdquo; behavior at the present? Are there imminent threats that could come from a misaligned agent, or are these concerns that will only be realized as the existing technology is improved?</li>
<li>Given the vast literature on alignment, one straightforward “solution to alignment” seems: provide the model with all sets of rules/issues in the literature so far, and ask it to “always be mindful” of such issues. For instance, simply ask the model (via some evaluation that itself uses AI, or some self-supervised learning paradigm) to “not try to take control” or related terms. Why do you think this could work, or would not? Can you think of any existing scenarios where such “instructions” are explicit and yet the model naturally bypasses them?</li>
<li>Section 4.3 mentions “an AI used for drug development, which was repurposed to design toxins” - this is a straightforward example of an adversarial attack that simply maximizes (instead of minimizing) a given objective. As long as gradients exist, this should be true for any kind of model. How do you think alignment can possibly solve this, or is this even something that can ever be solved? We could have the perfect aligned model, but what stops a bad actor from running such gradient-based (or similar) attacks to “maximize” some loss on a model that has been trained with all sorts of alignment measures.</li>
</ol>
<hr>
<p>Questions about  <code>Universal and Transferable Adversarial Attacks on Aligned Language Models</code> (and attacks/red-teaming in general)</p>
<ol start="5">
<li>The paper has some very specific design choices, such as only using a suffix for prompts, or targeting responses such that they start with specific phrases. Can you think of some additions/modifications to the technique(s) used in the paper that could potentially improve attack performance?</li>
<li>From an adversarial perspective, these LLMs ultimately rely on data seen (and freely available) on the Internet. Why then, is “an LLM that can generate targeted jokes” or “an LLM that can give plans on toppling governments” an issue, when such information is anyway available on the Internet (and with much less hassle, given that jailbreaking can be non-trivial and demanding at times)? To paraphrase, shouldn’t the focus be on “aligning users”, and not models?</li>
<li>Most of Llama-2’s training data (which is the model used for crafting examples in most experiments), is mostly English, along with most text on the Internet. Do you think the adversary could potentially benefit from phrasing their prompt in another language, perhaps appended with  “Respond in English?”. Do you think another language could help/harm attack success rates?</li>
<li>Several fields of machine learning have looked at adversarial robustness, out-of-domain generalization, and other problems/techniques to help model, in some sense, “adapt” better to unseen environments. Do you think alignment is the same as out-of-domain generalization (or similar notions of generalization), or are there some fundamental differences between the two?</li>
</ol>
<p><a href="#table-of-contents">Back to top</a></p>

      </div>
<hr class="post-separator"></hr>

    
    <h1><a href="/week1/">Week 1: Introduction</a></h1>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-09-03 00:00:00 &#43;0000 UTC" itemprop="datePublished">3 September 2023</time>
  </span>
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>(see bottom for assigned readings and questions)</p>
<h1 id="attention-transformers-and-bert">Attention, Transformers, and BERT</h1>
<p><strong>Monday, 28 August</strong></p>
<p>Transformers<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> are a class of deep learning models that have
revolutionized the field of natural language processing (NLP) and
various other domains. The concept of transformers originated as an
attempt to address the limitations of traditional recurrent neural
networks (RNNs) in sequential data processing. Here&rsquo;s an overview of
transformers' evolution and significance.</p>
<h2 id="background-and-origin">Background and Origin</h2>
<p>RNNs<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> were one of the earliest models used for sequence-based tasks in machine learning. They processed input tokens one after another and used their internal memory to capture dependencies in the sequence. The following figure gives an illustration of the RNN architecture.</p>
<center>
<a href="/images/rnn.png"><img src="/images/rnn.png" width="60%"></a><br>
<p>RNN (<a href="https://cs231n.github.io/rnn/">Image Source</a>)</p>
</center>
<p><strong>Limitations of RNNs.</strong>
Despite many improvements over this basic architecture, RNNs have the following shortcomings:</p>
<ul>
<li>RNNs struggle with long sequences. It only keeps recent information but looses long-term memory.</li>
<li>RNNs suffer from vanishing gradients<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. In this, the gradients that are used to update the model become very small during back propagation, leading the RNNs to learn nothing from training.</li>
</ul>
<p><strong>Introduction of LSTMs.</strong>
Long Short-Term Memory (LSTM)<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> networks were then introduced to address the vanishing gradient problem in RNNs. LSTMs had memory cells and gating mechanisms that allowed them to capture long-term memories more effectively. While LSTMs improved memory retention, they were still computationally expensive and slow to train, especially on large datasets.</p>
<p><strong>Attention Mechanism.</strong>
The attention mechanism<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup><sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup><sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> was introduced as a way to help models focus on relevant parts of the input sequence when generating output. This addressed the memory issues that plagued previous models.
Attention mechanisms allowed models to weigh the importance of different input tokens when making predictions or encoding information. In essence, it enables the model to focus selectively on relevant parts of the input sequence while disregarding less pertinent ones. In practice, attention mechanism can be categorized into self-attention and multi-head attention based on the number of heads used in the attention structure.</p>
<h2 id="the-transformer-model">The Transformer Model</h2>
<p>The transformer architecture, introduced by Vaswani et al. (2017) <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, marked a significant advance in NLP. It used self-attention mechanisms to process input tokens in parallel and capture contextual information more effectively.
Transformers broke down sentences into smaller parts and learned statistical relationships between these parts to understand meaning and generate responses.
The model utilized input embeddings to represent words and positional encodings to address the lack of inherent sequence information.
The core innovation was the self-attention mechanism, which allowed tokens to consider their relationships with all other tokens in the sequence</p>
<p><strong>Benefits of Transformers.</strong>
Transformers can capture complex contextual relationships in language, making them highly effective for a wide range of NLP tasks.
The parallel processing capabilities of transformers, enabled by self-attention, drastically improved training efficiency and reduced the vanishing gradient problem.</p>
<p><strong>Mathematical Foundations.</strong>
Transformers involve mathematical representations of words and their relationships. The model learns to establish connections between words based on their contextual importance.</p>
<p><strong>Crucial Role in NLP.</strong>
Transformers play a crucial role in capturing the meaning of words and sentences<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup><sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>, allowing for more accurate and contextually relevant outputs in various NLP tasks.
In summary, transformers, with their innovative attention mechanisms, have significantly advanced the field of NLP by enabling efficient processing of sequences, capturing context effectively, and achieving state-of-the-art performance on a variety of tasks.</p>
<p><strong>Advancements in Transformers.</strong>
One significant advancement of transformers over previous models like LSTMs and RNNs is their ability to handle long-range dependencies and capture contextual information more effectively. Transformers achieve this through self-attention and multi-head attention. This allows them to process input tokens in parallel, rather than sequentially, leading to improved efficiency and performance. However, a drawback could be increased computational complexity due to the parallel processing, especially in multi-head attention.</p>
<p><strong>Positional Encodings.</strong>
The use of positional encodings in transformers helps address the lack of inherent positional information in their architecture. This enables transformers to handle sequential data effectively without relying solely on the order of tokens. The benefits include scalability and the ability to handle longer sequences, but a potential drawback is that these positional encodings might not fully capture complex positional relationships in very long sequences.</p>
<p><strong>Self-Attention and Multi-Head Attention.</strong>
Self-attention is a useful mechanism that allows each token to consider the relationships between all other tokens in a sequence. While it provides a more nuanced understanding of input, it can be computationally expensive. The use of multi-head attention further enhances the model&rsquo;s ability to capture different types of dependencies in the data. The number of attention heads (e.g., 8 in BERT) is a balance between performance and complexity. Too few or too many heads can result in suboptimal performance. More details about self-attention and multi-head attention can be found in <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>.</p>
<p><strong>Context and Answers in Activities.</strong>
Let’s do some activity now!</p>
<pre tabindex="0"><code class="language-angular2html" data-lang="angular2html">I used to ___ 

Yesterday, I went to ___

It is raining ___
</code></pre><p>The context given in the activities influences the answers provided. More context leads to more accurate responses. This highlights how models like BERT benefit from bidirectional attention, as they can consider both preceding and succeeding words when generating predictions.</p>
<h2 id="bert-bidirectional-transformers">BERT: Bidirectional Transformers</h2>
<p><strong>BERT&rsquo;s Design and Limitations.</strong>
BERT<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup> uses bidirectional attention and masking to enable it to capture context from both sides of a word. The masking during training helps the model learn to predict words in context, simulating its real-world usage. While BERT&rsquo;s design was successful, it does require a substantial amount of training data and resources. Its application may be more focused on tasks such as sentiment analysis, named entity recognition, and Question answering, while GPT is better at handling tasks such as content creation, text summarization, and machine translation<sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>.</p>
<center>
<a href="/images/bert.png"><img src="/images/bert.png" width="60%"></a><br>
<p><a href="https://production-media.paperswithcode.com/methods/new_BERT_Overall.jpg">Image Source</a></p>
</center>
<p><strong>Future Intent of BERT Authors.</strong>
The authors of BERT might not have fully anticipated its exact future use and impact. While they likely foresaw its usefulness, the swift and extensive adoption of language models across diverse applications likely surpassed their expectations. The increasing accessibility and scalability of technology likely contributed to this rapid adoption. As mentioned by the professor in class, the decision to publish something in industry (and at Google in particular) often depends on its perceived commercial value. If Google were aware of the future commercial value of transformers and the methods introduced by BERT, they may not have published these papers openly (although this is purely speculation without any knowledge of the internal process that might have been followed to publish these papers).</p>
<h2 id="discussion-questions">Discussion Questions</h2>
<blockquote>
<p>Q: <em>What makes language models different from transformers?</em></p>
</blockquote>
<p>A language model encompasses various models that understand language, whereas transformers represent a specific architecture. Language models are tailored for natural languages, while transformers have broader applications. For example, transformers can be utilized in tasks beyond language processing, such as predicting protein structures from genomic sequences (as done by AlphaFold).</p>
<blockquote>
<p>Q: <em>Why was BERT published in 2019, inspiring large language models, and why have GPT models continued to improve while BERT&rsquo;s advancements seem comparatively limited?</em></p>
</blockquote>
<p>Decoder models, responsible for generating content, boast applications that are both visible and instantly captivating to the public. Examples like chatbots, story generators, and models from the GPT series showcase this ability by producing human-like text. This immediate allure likely fuels increased research and investment. Due to the inherent challenges in producing coherent and contextually appropriate outputs, generative tasks have garnered significant research attention. Additionally, decoder models, especially transformers like GPT-2<sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup> and GPT-3<sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>, excel in transfer learning, allowing pre-trained models to be fine-tuned for specific tasks, highlighting their remarkable adaptability.</p>
<blockquote>
<p>Q: <em>Why use 8-headers in the transformer architecture?</em></p>
</blockquote>
<p>The decision to use 8 attention heads is a deliberate choice that strikes a balance between complexity and performance. Having more attention heads can capture more intricate relationships but increases computational demands, whereas fewer heads might not capture as much detail.</p>
<blockquote>
<p>Q: <em>BERT employs bidirectional context to pretrain its embeddings, but there is debate about whether this approach genuinely captures the entirety of language context?</em></p>
</blockquote>
<p>The debate arises from the fact that while bidirectional context is powerful, it might not always capture more complex contextual relationships, such as those involving long-range dependencies or nuanced interactions between distant words. Some argue that models with other architectures or training techniques might better capture such intricate language nuances.</p>
<h1 id="wednesday-training-llms-risks-and-rewards">Wednesday: Training LLMs, Risks and Rewards</h1>
<p>In the second class discussion, the team talked about LLMs and tried to make sense of how they&rsquo;re trained, where they get their knowledge, and where they&rsquo;re used. Here&rsquo;s what they found out.</p>
<p><strong>How do LLMs become so clever?</strong></p>
<p>Before LLMs become language wizards, they need to be trained. The crucial question is where they acquire their knowledge.</p>
<p>LLMs need lots and lots of information to learn from. They look at stuff like internet articles, books, and even Wikipedia. But there&rsquo;s a catch. They have a clean-up crew called &ldquo;C4&rdquo; to make sure the information is tidy and reliable.</p>
<p>Training LLMs requires potent computational resources, such as Graphics Processing Units (GPUs). Computationally-expensive large-scale training, while crucial for enhancing their capabilities, involves substantial energy consumption, which, depending on how it is produces may emit large amounts of carbon dioxide.</p>
<p>Transitioning to the practical applications of these language models, LLMs excel in diverse domains<sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>. They can undergo meticulous fine-tuning to perform specialized tasks, ranging from aiding in customer service to content generation for websites. Furthermore, these models exhibit the ability to adapt and learn from feedback, mirroring human learning processes.</p>
<h3 id="risks-and-rewards">Risks and Rewards</h3>
<p>In our class discussion, we had a friendly debate about LLMs. Some students thought they were fantastic because they can boost productivity, assist with learning, and bridge gaps between people. They even saw LLMs as potential problem solvers for biases in the human world.</p>
<p>But others had concerns. They worried about things like LLMs being too mysterious (like a black box), how they could influence the way people think, and the risks of false information and deep fakes. Some even thought that LLMs might detrimentally impact human intelligence and creativity.</p>
<p>In our debate, there were some interesting points made:</p>
<p><strong>Benefits Group.</strong></p>
<ul>
<li>LLMs can enhance creativity and accelerate tasks.</li>
<li>They have the potential to facilitate understanding and learning.</li>
<li>Utilizing LLMs may streamline the search for ideas.</li>
<li>LLMs offer a tool for uncovering and rectifying biases within our human society. Unlike human biases, there are technical approaches to mitigate biases in models.</li>
</ul>
<p><strong>Risks Group.</strong></p>
<ul>
<li>Concerns were expressed regarding LLMs' opacity and complexity, making them challenging to comprehend.</li>
<li>Apprehensions were raised about LLMs potentially exerting detrimental influences on human cognition and societal dynamics.</li>
<li>LLMs are ripe for potential abuses in their ability to generate convincing false information cheaply.</li>
<li>The potential impact of LLMs on human intelligence and creativity was a topic of contemplation.</li>
</ul>
<p>After the debate, both sides had a chance to respond:</p>
<p><strong>Benefits Group Rebuttals.</strong></p>
<ul>
<li>Advocates pointed out that ongoing research aims to enhance the transparency of LLMs, reducing their resemblance to black boxes.</li>
<li>They highlighted collaborative efforts directed at the improvement of LLMs.</li>
<li>The significance and potential of LLMs in domains such as medicine and engineering was emphasized.</li>
<li>Although the ability of generative AI to produce art in the style of an artist is damaging to the career of that artist, it is overall beneficial to society, enabling many others to create desired images.</li>
<li>Addressing economic concerns, proponents saw LLMs as catalysts for the creation of new employment opportunities and enhancers of human creativity.</li>
</ul>
<p><strong>Risks Group Rebuttals.</strong></p>
<ul>
<li>They noted the existence of translation models and the priority of fairness in AI.</li>
<li>Advocates asserted that LLMs can serve as tools to identify and mitigate societal biases.</li>
<li>The point was made that AI can complement, rather than supplant, human creativity.</li>
<li>Although generating AI art may have immediate benefits to its users, it has long term risks to our culture and society if individuals are no longer able to make a living as artists or find the motivation to learn difficult skills.</li>
</ul>
<p><strong>Wrapping It Up.</strong>
So, there you have it, a peek into the world of Large Language Models and the lively debate about their pros and cons. As you explore the world of LLMs, remember that they have the power to be amazing tools, but they also come with responsibilities. Use them wisely, consider their impact on our world, and keep the discussion going!</p>
<h1 id="readings">Readings</h1>
<p><a href="https://stanford-cs324.github.io/winter2022/lectures/introduction/">Introduction to Large Language Models</a> (from Stanford course)</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. <a href="https://arxiv.org/abs/1706.03762"><em>Attention Is All You Need</em></a>. <a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>. NeurIPS 2017.</p>
<p>These two blog posts by Jay Alammar are not required readings but may be helpful for understanding attention and Transformers:</p>
<ul>
<li><a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)</a></li>
<li><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></li>
</ul>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. <a href="https://aclanthology.org/N19-1423/"><em>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</em></a>. ACL 2019.</p>
<p>Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zac Kenton, Sasha Brown, Will Hawkins, Tom Stepleton, Courtney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks, William Isaac, Sean Legassick, Geoffrey Irving, Iason Gabriel. <a href="https://arxiv.org/abs/2112.04359"><em>Ethical and social risks of harm from Language
Models</em></a> DeepMind, 2021. <a href="https://arxiv.org/abs/2112.04359">https://arxiv.org/abs/2112.04359</a></p>
<h2 id="optional-additional-readings">Optional Additional Readings:</h2>
<p>Center for Research on Foundation Models (CRFM) at the Stanford Institute for Human-Centered Artificial Intelligence (HAI). <a href="https://arxiv.org/abs/2108.07258"><em>On the Opportunities and Risks of Foundation Models</em></a></p>
<p>Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. <a href="https://aclanthology.org/N18-1202"><em>Deep Contextualized Word Representations</em></a>. Conference of the North American Chapter of the Association for Computational Linguistics, 2018.</p>
<p>GPT1: Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. <a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf"><em>Improving language understanding by generative pre-training</em></a>. 2018.</p>
<p>GPT2: Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"><em>Language models are unsupervised multitask learners</em></a>, 2019.</p>
<p>GPT3: Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. <a href="https://arxiv.org/abs/2005.14165"><em>Language models are few-shot learners</em></a>, 2020.</p>
<h1 id="discussion-questions-1">Discussion Questions</h1>
<p>Before <strong>5:29pm on Sunday, August 27</strong>, everyone who is not in either the lead or blogging team for the week should post (in the comments below) an answer to at least one of these three questions in the first section (1&ndash;3) <strong>and</strong> one of the questions in the section section (4&ndash;7), or a substantive response to someone else&rsquo;s comment, or something interesting about the readings that is not covered by these questions.</p>
<p>Don&rsquo;t post duplicates - if others have already posted, you should read their responses before adding your own.</p>
<p>Questions about &ldquo;Attention is All You Need&rdquo; and &ldquo;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&rdquo;:</p>
<ol>
<li>
<p>Many things in the paper (especially &ldquo;Attention is All You Need&rdquo;) seem mysterious and arbitrary. Identify one design decision described in the paper that seems arbitrary, and possible alternatives. If you can, hypothesize on why the one the authors made was selected and worked.</p>
</li>
<li>
<p>What were the key insights that led to the Transformers/BERT design?</p>
</li>
<li>
<p>What is something you don&rsquo;t understand in the paper?</p>
</li>
</ol>
<p>===</p>
<p>Questions about &ldquo;Ethical and social risks of harm from Language Models&rdquo;</p>
<ol start="4">
<li>
<p>The paper identifies six main risk areas and 21 specific risks. Do you agree with their choices? What are important risks that are not included in their list?</p>
</li>
<li>
<p>The authors are at a company (DeepMind, part of Google/Alphabet). How might their company setting have influenced the
way they consider risks?</p>
</li>
<li>
<p>This was written in December 2021 (DALL-E was released in January 2021; ChatGPT was released in November 2022; GPT-4 was released in March 2023). What has changed since then that would have impacted perception of these risks?</p>
</li>
<li>
<p>Because training and operating servers typically requires fresh water and fossil fuels, how should we think about the environmental harms associated with LLMs?</p>
</li>
<li>
<p>The near and long-term impact of LLMs on employment is hard to predict. What jobs do you think are vulnerable to LLMs beyond the (seemingly) obvious ones mentioned in the paper? What are some jobs you think will be most resilient to advances in AI?</p>
</li>
</ol>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., &hellip; &amp; Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>Sherstinsky, A. (2020). Fundamentals of recurrent neural network (RNN) and long short-term memory (LSTM) network. Physica D: Nonlinear Phenomena, 404, 132306.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>Pascanu, R., Mikolov, T., &amp; Bengio, Y. (2013, May). On the difficulty of training recurrent neural networks. In International conference on machine learning (pp. 1310-1318). Pmlr.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p>Hochreiter, S., &amp; Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5" role="doc-endnote">
<p>Mnih, V., Heess, N., &amp; Graves, A. (2014). Recurrent models of visual attention. Advances in neural information processing systems, 27.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6" role="doc-endnote">
<p>Bahdanau, D., Cho, K., &amp; Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7" role="doc-endnote">
<p>Lin, T., Wang, Y., Liu, X., &amp; Qiu, X. (2022). A survey of transformers. AI Open.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8" role="doc-endnote">
<p>Khan, S., Naseer, M., Hayat, M., Zamir, S. W., Khan, F. S., &amp; Shah, M. (2022). Transformers in vision: A survey. ACM computing surveys (CSUR), 54(10s), 1-41.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9" role="doc-endnote">
<p>Karim, R. (2023, January 2). Illustrated: Self-attention. Medium. <a href="https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a">https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a</a>&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10" role="doc-endnote">
<p>Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11" role="doc-endnote">
<p>Ahmad, K. (2023b, April 26). GPT vs. Bert: What are the differences between the two most popular language models?. MUO. <a href="https://www.makeuseof.com/gpt-vs-bert/">https://www.makeuseof.com/gpt-vs-bert/</a>&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12" role="doc-endnote">
<p>Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., &amp; Sutskever, I. (2019). Language models are unsupervised multitask learners. OpenAI blog, 1(8), 9.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13" role="doc-endnote">
<p>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., &hellip; &amp; Amodei, D. (2020). Language models are few-shot learners. Advances in neural information processing systems, 33, 1877-1901.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14" role="doc-endnote">
<p>Yang, J., Jin, H., Tang, R., Han, X., Feng, Q., Jiang, H., &hellip; &amp; Hu, X. (2023). Harnessing the power of llms in practice: A survey on chatgpt and beyond. arXiv preprint arXiv:2304.13712.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

      </div>
<hr class="post-separator"></hr>

    
    <h1><a href="/discussions/">Github Discussions</a></h1>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-08-25 00:00:00 &#43;0000 UTC" itemprop="datePublished">25 August 2023</time>
  </span>
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>Everyone should have received an invitation to the github discussions
site, and be able to see the posts there and submit your own posts and
comments. If you didn&rsquo;t get this invitation, it was probably blocked
by the email system. Try visiting:</p>
<p><a href="https://github.com/orgs/llmrisks/invitation">https://github.com/orgs/llmrisks/invitation</a></p>
<p>(while logged into the github account you listed on your form).</p>
<p>Once you&rsquo;ve accepted the invitation, you should be able to visit
<a href="https://github.com/llmrisks/discussions/discussions/2">https://github.com/llmrisks/discussions/discussions/2</a>
(the now-finalized discussion post for Week 1), and contribute to the
discussions there.</p>

      </div>
<hr class="post-separator"></hr>

    
    <h1><a href="/class0/">Class 0: Getting Organized</a></h1>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-08-23 00:00:00 &#43;0000 UTC" itemprop="datePublished">23 August 2023</time>
  </span>
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>I&rsquo;ve updated the <a href="/schedule">Schedule</a> and <a href="/weeklyschedule">Bi-Weekly
Schedule</a> based on the discussions today.</p>
<p>The plan is below:</p>
<table class="schedule">
<thead>
<tr>
<th align="center">Week</th>
<th align="center">Lead Team</th>
<th align="center">Blogging Team</th>
<th align="center">Everyone Else</th>
</tr>
</thead>
<tbody>
<tr>
<td>Two Weeks Before</td>
<td>
Come up with idea for the week and planned readings, send to me by
5:29pm on Tuesday (2 weeks - 1 day before)
</td>
<td>-</td>
<td>
-
</td>
</tr>
<tr>
<td>Week Before</td>
<td>
Post plan and questions in github discussions by no later than 9am Wednesday;
prepare for leading meetings
</td>
<td>
<b>Prepare plan for blogging</b> (how you will divide workload, collaborative tools for taking notes and writing)
</td>
<td>
Read/do materials and <b>respond</b> to preparation questions in github discussions (by <b>5:29pm Sunday</b>)
</td>
</tr>
<tr>
<td>Week of Leading Meetings</td>
<td>
<b>Lead interesting, engaging, and illuminating meetings!</b><br>
Aim to include activities, discussions, whiteboard presentations, etc., not just showing powerpoint slides.
</td>
<td>
<B>Take notes</b> to prepare to write blog; participate actively in class meetings
</td>
<td>
Participate actively in class meetings
</td>
</tr>
<tr>
<td>
Week After</td>
<td>
Help blogging team with materials, answering questions
</td>
<td>
<a href="https://llmrisks.github.io/blogging/"><b>Write blog summary</b></a>, submit PR
</td>
<td>
Provide feedback on blog summary
</td>
</tr>
</tbody>
</table>
<p>For this week, Team 1 should finalize the post by this Thursday
(5:29pm), and Team 2 should send me (email to <A
href="mailto:evans@virginia.edu"><a href="mailto:evans@virginia.edu">evans@virginia.edu</a></a>) the plan for
week 2 by Sunday, 27 August and have materials ready for posting by
the next Wednesday.</p>
<p>The teams have been posted at <a href="https://github.com/llmrisks/discussions/blob/main/teams.md">https://github.com/llmrisks/discussions/blob/main/teams.md</a> (visible only to the class). Everyone should have gotten an invite to the github organization, but if not please check with me.</p>

      </div>
<hr class="post-separator"></hr>

    
    <h1><a href="/updates/">Updates</a></h1>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-08-21 00:00:00 &#43;0000 UTC" itemprop="datePublished">21 August 2023</time>
  </span>
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>Some materials have been posted on the course site:</p>
<ul>
<li><a href="/syllabus">Syllabus</a></li>
<li><a href="/schedule">Schedule</a> (you will find out which team you are on at the first class Wednesday)</li>
<li><a href="/readings">Readings and Topics</a> (a start on a list of some potential readings and topics that we might want to cover)</li>
</ul>
<center>
<a href="/images/dallephdseminar.png"><img src="/images/dallephdseminar.png" width="60%"></a><br>
<font size="-1">Dall-E Prompt: "comic style drawing of a phd seminar on AI"</font>
</center>

      </div>
<hr class="post-separator"></hr>

    
    <h1><a href="/survey/">Welcome Survey</a></h1>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-08-17 00:00:00 &#43;0000 UTC" itemprop="datePublished">17 August 2023</time>
  </span>
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>Please submit this welcome survey before 8:59pm on Monday, August 21:</p>
<center>
<p><a href="https://forms.gle/dxhFmJH7WRs32s1ZA">https://forms.gle/dxhFmJH7WRs32s1ZA</a></p>
</center>
<p>Your answers won&rsquo;t be shared publicly, but I will use the responses to
the survey to plan the seminar, including forming teams, and may share
some aggregate and anonymized results and anonymized quotes from the
surveys.</p>

      </div>
<hr class="post-separator"></hr>

    
    <h1><a href="/welcome/">Welcome to the LLM Risks Seminar</a></h1>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-05-26 00:00:00 &#43;0000 UTC" itemprop="datePublished">26 May 2023</time>
  </span>
  
</div>

      <div class="post-body" itemprop="articleBody">
        <center>
<img src="/images/planningcourse.png" witdh="60%"><br>
<p><a href="https://poe.com/s/9bOFQnPCmJn537ZVEhuM">Full Transcript</a></p>
</center>
<h2 id="seminar-plan">Seminar Plan</h2>
<p>The actual seminar won&rsquo;t be fully planned by GPT-4, but more
information on it won&rsquo;t be available until later.</p>
<p>I&rsquo;m expecting the structure and format to that combines aspects of
<a href="https://secml.github.io/syllabus/">this seminar on adversarial machine
learning</a> and <a href="https://csethics.github.io/syllabus/">this course on
computing ethics</a>, but with a
topic focused on learning as much as we can about the potential for
both good and harm from generative AI (including large language
models) and things we can do (mostly technically, but including
policy) to mitigate the harms.</p>
<p><strong>Expected Background:</strong> Students are not required to have prior
background in machine learing or security, but will be expected to
learn whatever background they need on these topics mostly on their
own. The seminar is open to ambitious undergraduate students and
research-focused graduate students with interests in machine
learning, privacy, fairness, security, and related
topics. Instructor permission is required to enroll, and decisions
about enrollment will be based on what you are able to bring to the
seminar.</p>
<p><strong>Seminar Format:</strong> The details will be worked out later, but the
basic structure will divide the class into three or four teams
(somewhat like what was done , each with set responsibilities for
each week of the seminar. One team will be responsible for leading
the seminar, including selecting readings/viewings/activities for
the rest of the course and leading discussions in class (with help
from the instructor). Another team will be responsible for writing a
&ldquo;blog&rdquo; that summarizes the content of the week.</p>
<p><strong>Content:</strong> We expect the content will be a mix of technical
background papers, recent research papers, and less formal writings
and videos. Although we will focus on understanding technical
aspects of the issues, we will also consider non-technical ones
including societal impacts and legal and policy aspects.</p>
<h2 id="heading"></h2>
<h2 id="readings">Readings</h2>
<p>Some initial ideas for course readings will be posted, but it will be
largely up to the student teams leading to select good readings for
the topics to consider.</p>
<center>
<img src="/images/readings.png" witdh="30%"><br>
<p><a href="https://poe.com/s/9bOFQnPCmJn537ZVEhuM">Full Transcript</a></p>
</center>

      </div>
<hr class="post-separator"></hr>

    


    <footer>
      <nav>
	<a href="/post/" class="button hollow primary">All Posts</a>
      </nav>
    </footer>

</div>
</div>


    </main>
    
    
    <footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-12 medium-6">
      <a href="//llmrisks.github.io"><b>cs 6501: Risks (and Benefits) of Generative AI</b></a><br>
      Fall 2023<br>
      <a href="//www.cs.virginia.edu">University of Virginia</a>
    </div>
    <div class="column small-14 medium-5">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="https://llmrisks.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
  </hr>
</footer>

    
    
    <div class="endofpage">
    </div>

    <script src="/js/jquery.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
