+++
date = "21 Aug 2023"
draft = false
title = "Readings and Topics"
slug = "readings"
+++

This page collects some potential topics and readings for the seminar.

## Introduction (Week 1)

[Introduction to Large Language Models](https://stanford-cs324.github.io/winter2022/lectures/introduction/) (from Stanford course)

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. [_Attention Is All You Need_](https://arxiv.org/abs/1706.03762). [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762). NeurIPS 2017.

Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, Robert McHardy. [_Challenges and Applications of Large Language Models_](https://arxiv.org/abs/2307.10169). [https://arxiv.org/abs/2307.10169](https://arxiv.org/abs/2307.10169)

Simon Willison. [_Catching up on the weird world of LLMs_](https://simonwillison.net/2023/Aug/3/weird-world-of-llms/), August 2023.

### Viewpoint Essays

Douglas Hofstadter. [_Gödel, Escher, Bach, and AI_](https://archive.is/2VtiC). The Atlantic. 8 July 2023.

Marc Andreessen. [AI Will Save the World](https://www.thefp.com/p/why-ai-will-save-the-world). 11 July 2023

Paul Kingsnorth. [_Rage Against the Machine_](https://www.thefp.com/p/rage-against-the-machine-ai-paul-kingsnorth). 12 July 2023



## Copyright and Law

Katherine Lee, A. Feder Cooper, James Grimmelmann, and Daphne Ippolito. [_AI and Law: The Next Generation_](https://genlaw.github.io/explainers/explainers.pdf). July 2023. [https://genlaw.github.io/explainers/](https://genlaw.github.io/explainers/)

[Sarah Silverman, Christopher Golden, and Richard Kadrey vs. OpenAI](https://s3.documentcloud.org/documents/23869693/silverman-openai-complaint.pdf). Legal Complaint against ChatGPT, file 7 July 2023.


Nikhil Vyas, Sham Kakade, Boaz Barak. [_On Provable Copyright Protection for Generative Models_](https://arxiv.org/abs/2302.10870). [https://arxiv.org/abs/2302.10870](https://arxiv.org/abs/2302.10870).


## Governance and Regulation

Michael Veale, Kira Matus, Robert Gorwa. [_AI and Global Governance: Modalities, Rationales, Tensions_](https://discovery.ucl.ac.uk/id/eprint/10171121/1/Veale%20Matus%20Gorwa%202023.pdf). Annual Review of Law and Social Science, 2023.


## Amplification Techniques

Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, Ahmed Awada. [_Orca: Progressive Learning from Complex Explanation Traces of GPT-4_](https://arxiv.org/abs/2306.02707). [https://arxiv.org/abs/2306.02707](https://arxiv.org/abs/2306.02707)

## Programming with LLMs

## Performance of LLMs

Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin,
Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan
Perez, Evan Hubinger, Kamilė Lukošiūtė, Karina Nguyen, Nicholas
Joseph, Sam McCandlish, Jared Kaplan, Samuel R. Bowman. [_Studying
Large Language Model Generalization with Influence
Functions_](https://arxiv.org/abs/2308.03296). [https://arxiv.org/abs/2308.03296](https://arxiv.org/abs/2308.03296)

## Integrations

Ernest Davis, Scott Aaronson. [_Testing GPT-4 with Wolfram Alpha and Code Interpreter plug-ins on math and science problems_](https://arxiv.org/abs/2308.05713). [https://arxiv.org/abs/2308.05713](https://arxiv.org/abs/2308.05713)



## Hallucination

Andrew Slavin Ross, Michael C. Hughes, Finale Doshi-Velez. [_Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations_](https://arxiv.org/abs/1703.03717). IJCAI 2017.

Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, Igor Mordatch. [_Improving Factuality and Reasoning in Language Models through Multiagent Debate_](https://arxiv.org/abs/2305.14325) [https://arxiv.org/abs/2305.14325](https://arxiv.org/abs/2305.14325).

## Abuses of LLMs

Nicholas Carlini. [_A LLM Assisted Exploitation of AI-Guardian_](https://arxiv.org/abs/2307.15008). 

Andy Zou, Zifan Wang, J. Zico Kolter, Matt Fredrikson. [_Universal and Transferable Adversarial Attacks on Aligned Language Models_](https://arxiv.org/abs/2307.15043). [https://arxiv.org/abs/2307.15043](https://arxiv.org/abs/2307.15043).
[Project Website: https://llm-attacks.org/](https://llm-attacks.org/).

## Fairness and Bias

Shangbin Feng, Chan Young Park, Yuhan Liu, Yulia Tsvetkov. [_From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models_](https://arxiv.org/abs/2305.08283). ACL 2023.

Myra Cheng, Esin Durmus, Dan Jurafsky. [_Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models_](https://arxiv.org/abs/2305.18189). ACL 2023.


## "Alignment"

Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, Hang Li. [_Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment_](https://arxiv.org/abs/2308.05374). [https://arxiv.org/abs/2308.05374](https://arxiv.org/abs/2308.05374).


## AGI

Yejin Choi. [_The Curious Case of Commonsense Intelligence_](https://www.amacad.org/publication/curious-case-commonsense-intelligence). Daedalus, Spring 2022.

Konstantine Arkoudas. [_GPT-4 Can't Reason_](https://arxiv.org/abs/2308.03762). [https://arxiv.org/abs/2308.03762](https://arxiv.org/abs/2308.03762).

Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, Vered Shwartz. [_Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models_](https://arxiv.org/abs/2305.14763). 

Melanie Sclar, Sachin Kumar, Peter West, Alane Suhr, Yejin Choi, Yulia
Tsvetkov. [_Minding Language Models' (Lack of) Theory of Mind: A
Plug-and-Play Multi-Character Belief
Tracker_](https://arxiv.org/abs/2306.00924). ACL 2023

Boaz Barak. [_The shape of AGI: Cartoons and back of envelope_](https://windowsontheory.org/2023/07/17/the-shape-of-agi-cartoons-and-back-of-envelope/). July 2023.


## Art

## Writing

## "Cheating"

## Scaling

## Embeddings

## Prompt Engineering and "Jailbreaking"

Alexander Wei, Nika Haghtalab, Jacob Steinhardt. [_Jailbroken: How Does LLM Safety Training Fail?_](https://arxiv.org/abs/2307.02483). July 2023.



## "Safety"

## Sentience

Patrick Butlin, Robert Long, Eric Elmoznino, Yoshua Bengio, Jonathan Birch, Axel Constant, George Deane, Stephen M. Fleming, Chris Frith, Xu Ji, Ryota Kanai, Colin Klein, Grace Lindsay, Matthias Michel, Liad Mudrik, Megan A. K. Peters, Eric Schwitzgebel, Jonathan Simon, Rufin VanRullen. [_Consciousness in Artificial Intelligence: Insights from the Science of Consciousness_](https://arxiv.org/abs/2308.08708). August 2023.

## Memorization and Inference Privacy

R. Thomas McCoy, Paul Smolensky, Tal Linzen, Jianfeng Gao, Asli Celikyilmaz. [_How Much Do Language Models Copy From Their Training Data? Evaluating Linguistic Novelty in Text Generation Using RAVEN_](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00567/116616/How-Much-Do-Language-Models-Copy-From-Their). TACL 2023. 


## Preventing Learning

Shawn Shan, Jenna Cryan, Emily Wenger, Haitao Zheng, Rana Hanocka, Ben
Y. Zhao. [_Glaze: Protecting Artists from Style Mimicry by
Text-to-Image
Models_](http://people.cs.uchicago.edu/~ravenben/publications/pdf/glaze-usenix23.pdf). USENIX Security 2023.  [Glaze Project Website](https://glaze.cs.uchicago.edu/) [https://arxiv.org/abs/2302.04222](https://arxiv.org/abs/2302.04222)

Pedro Sandoval-Segura, Vasu Singla, Jonas Geiping, Micah Goldblum, Tom
Goldstein. [_What Can We Learn from Unlearnable
Datasets?_](https://arxiv.org/abs/2305.19254). [https://arxiv.org/abs/2305.19254](https://arxiv.org/abs/2305.19254)

Giannis Daras, Kulin Shah, Yuval Dagan, Aravind Gollakota, Alexandros
G. Dimakis, Adam Klivans.  [_Ambient Diffusion: Learning Clean
Distributions from Corrupted Data_](https://arxiv.org/abs/2305.19256). [https://arxiv.org/abs/2305.19256](https://arxiv.org/abs/2305.19256)

## Training on Generated Data

Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, Ross Anderson. [_The Curse of Recursion: Training on Generated Data Makes Models Forget_](https://arxiv.org/abs/2305.17493). [https://arxiv.org/abs/2305.17493](https://arxiv.org/abs/2305.17493)

Sina Alemohammad, Josue Casco-Rodriguez, Lorenzo Luzi, Ahmed Imtiaz Humayun, Hossein Babaei, Daniel LeJeune, Ali Siahkoohi, Richard G. Baraniuk. [_Self-Consuming Generative Models Go MAD_](https://arxiv.org/abs/2307.01850). [https://arxiv.org/abs/2307.01850](https://arxiv.org/abs/2307.01850).


## Useful Guides

[_How to Use AI to Do Stuff: An Opinionated Guide_](https://www.oneusefulthing.org/p/how-to-use-ai-to-do-stuff-an-opinionated) (Ethan Mollick)

[OpenAI Cookbook](https://github.com/openai/openai-cookbook)


### More Sources

[Awesome-LLM: a curated list of Large Language Model Papers and Links](https://github.com/Hannibal046/Awesome-LLM)

[COS 597G (Fall 2022): Understanding Large Language Models](https://www.cs.princeton.edu/courses/archive/fall22/cos597G/) (Princeton Course taught by [Danqi Chen](https://www.cs.princeton.edu/~danqic/)

[CS324 - Large Language Models (Winter 2022)](https://stanford-cs324.github.io/winter2022/) (Stanford Course taught by [Percy Liang](https://cs.stanford.edu/~pliang/), [
Tatsunori Hashimoto](https://thashim.github.io/), and [Christopher Ré](https://cs.stanford.edu/~chrismre/))